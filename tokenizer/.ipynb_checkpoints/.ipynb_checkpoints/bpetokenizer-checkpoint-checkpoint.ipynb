{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9a3844-fb2d-456d-b317-8cba5bdb71d8",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding Tokenizer\n",
    "---\n",
    "Following Andrej Karpathy's [video](https://www.youtube.com/watch?v=zduSFxRajkE) for tokenizers, building my own BPE tokenizer. From this, will use it as a basis to train my own embedding model based on Word2Vec and analyze the results of that training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2894460-9169-435c-95b6-8020f50ca12b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1) Import Dependencies and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221d3285-ecb0-4501-8898-02a163cf44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import regex as re\n",
    "\n",
    "# Also defining our regex up here (from GPT-4 tokenizer)\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680c07fc-faf8-4d47-a1c6-b11f4db9a7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# First, need to import data. We'll use tiny shakespeare -- will be able to clearly see what kinds of words are commonly used in the ye olde times\n",
    "file_path = '../data/tinyshakespeare.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Print first 100 chars\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46bdd5d-0e8b-4388-826d-314bc318eb00",
   "metadata": {},
   "source": [
    "## 2) Defining our BPETokenizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91943fc-8063-4198-8740-88e844beffc7",
   "metadata": {},
   "source": [
    "### Approach\n",
    "---\n",
    "For our tokenizer, we'll create a class called BPETokenizer. Upon instantiation, it will take in a regex expression (optional). Subfunctions include:\n",
    "1. _encodetext: Takes in raw text and returns 2-D python list of ASCII values (if regex, split by regex; else just 2-D w/ 1 element)\n",
    "2. _mergepair: helper function to take in your encoded text and a tuple (pair, token) that replaces the pair with the desired token\n",
    "3. train(text, vocab_size): Function to iteratively convert most frequent byte-pairs to a new token. Saves encoding map to class\n",
    "4. encode(text): Converts text to tokens; returns as 1-d python list of tokens\n",
    "5. decode(tokens): Converts tokens to text; returns string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c7c1aab-d47b-4929-8fa1-94ac5a47164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "\n",
    "    def __init__(self, regex=None):\n",
    "        self.encoding_map = {}\n",
    "        self.decoding_map = {}\n",
    "        self.regex = regex\n",
    "\n",
    "    def regex_setter(self, re):\n",
    "        \"\"\" Simple setter function for regex \"\"\"\n",
    "        self.regex = re\n",
    "\n",
    "    def encmap_getter(self):\n",
    "        \"\"\" Simple getter to return our encoding map \"\"\"\n",
    "        return self.encoding_map\n",
    "        \n",
    "    def decmap_getter(self):\n",
    "        \"\"\" Simple getter to return our decoding map \"\"\"\n",
    "        return self.decoding_map\n",
    "\n",
    "    def _encodetext(self, text):\n",
    "        \"\"\"\n",
    "            Takes in raw text and returns 2-D python array of ASCII values (e.g., [[70, 105, 114, 115, 116], [32, 67,...]])\n",
    "            Replaces non-ASCII with '?'\n",
    "        \"\"\"\n",
    "        if self.regex == None:\n",
    "            text = [text]\n",
    "        else:\n",
    "            text = re.findall(self.regex, text)     # Converts text to ['First', ' Citizen', ':\\n', 'Before', ' we', ' proceed',...]\n",
    "        return list( list(t.encode(\"ascii\", errors=\"replace\")) for t in text )\n",
    "\n",
    "    def _mergepair(self, tokens, pair_tok):\n",
    "        \"\"\"\n",
    "            Takes in 2-D python list of tokens and iterates through, replacing 'tok' where 'pair' exists\n",
    "        \"\"\"\n",
    "        pair, tok = pair_tok\n",
    "        merged = []\n",
    "        replaced = False\n",
    "        for block in tokens:\n",
    "            merged_block = []\n",
    "            if len(block) <= 1:\n",
    "                merged_block.extend(block)    # Simply append element and move onto next block\n",
    "            else:\n",
    "                for idx in range(len(block)-1):\n",
    "                    if replaced:\n",
    "                        replaced = False   # Knows to skip next idx if you already replaced it with a bpe token \n",
    "                    elif block[idx]==pair[0] and block[idx+1]==pair[1]:\n",
    "                        merged_block.append(tok)\n",
    "                        replaced = True\n",
    "                    elif idx == len(block)-2:\n",
    "                        merged_block.extend([block[idx], block[idx+1]])\n",
    "                    else:\n",
    "                        merged_block.append(block[idx])\n",
    "            merged.append(merged_block)\n",
    "        return merged\n",
    "\n",
    "    def train(self, text, vocab_size):\n",
    "        \"\"\"\n",
    "            Takes in raw text and a desired length for max vocabulary size\n",
    "            Upon completion, sets encoder_map and decoder_map to the BPE mappings (forward, backward resp.) \n",
    "        \"\"\"\n",
    "        encoded_text = self._encodetext(text)\n",
    "        encoding_map = {}                 # Create python dictionary of merges\n",
    "        num_merges = vocab_size - 128     # Number of iterations of BPE\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            bytepair_count = {}\n",
    "            for block in encoded_text:\n",
    "                if len(block) > 1:\n",
    "                    for idx in range(len(block)-1):\n",
    "                        pair = (block[idx], block[idx+1])\n",
    "                        count = bytepair_count.get(pair, 0)\n",
    "                        bytepair_count[pair] = count+1\n",
    "            \n",
    "            # Once done iterating through all the ascii values, sort and assign most freq bytepair to new token\n",
    "            freq_pair = max(bytepair_count, key=bytepair_count.get)\n",
    "            new_token = 128 + i\n",
    "            encoding_map[freq_pair] = new_token\n",
    "            encoded_text = self._mergepair(encoded_text, (freq_pair, new_token))\n",
    "\n",
    "        self.encoding_map = encoding_map\n",
    "        self.decoding_map = {value: key for key, value in encoding_map.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "            Takes in raw text and returns tokenized text (1-D array)\n",
    "        \"\"\"\n",
    "        encodings = list(self.encoding_map.items())\n",
    "        encoded_text = self._encodetext(text)\n",
    "        for pair_tok in encodings:\n",
    "            encoded_text = self._mergepair(encoded_text, pair_tok)\n",
    "        tokenized_text = [tok for sublist in encoded_text for tok in sublist]\n",
    "        return tokenized_text\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"\n",
    "            Takes in tokenized text (1-D array) and returns raw text\n",
    "        \"\"\"\n",
    "        decodings = list(self.decoding_map.items())\n",
    "        for tok, pair in decodings[::-1]:\n",
    "            decoded_tokens = []\n",
    "            for idx, t in enumerate(tokens):\n",
    "                if t != tok:\n",
    "                    decoded_tokens.append(t)\n",
    "                    # print(t)\n",
    "                else:\n",
    "                    decoded_tokens.extend( [pair[0], pair[1]] )\n",
    "                    # print([pair[0], pair[1]])\n",
    "            tokens = decoded_tokens\n",
    "            # print(''.join(chr(value) for value in tokens))\n",
    "\n",
    "        decoded_text = ''.join(chr(value) for value in tokens)\n",
    "        return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7889d758-d1b0-4e9f-b477-d3c9cef816c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_text = data[:500]\n",
    "tokenizer = BPETokenizer(GPT4_SPLIT_PATTERN)\n",
    "tokenizer.train(trimmed_text, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bea59342-746f-4e07-bce2-04a1835f286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = tokenizer.encode(trimmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "542e6e74-ddce-4c0e-9132-81ac50f49d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Firs Citiz:\\nor w proceedny furthe, hear me speak.\\n\\nll:\\n, speak.\\n\\n Citiz:\\n allresolvedrathe t die than t famish?\\n\\nll:\\nsolved.resolved.\\n\\n Citiz:\\nrs,ou knowaius Marcius is chief enemy t thepeople.\\n\\nll:\\nnow, w know.\\n\\n Citiz:\\nus killhim, and w'llhave cor a our ow price.\\nIs't averdict?\\n\\nll:\\nmor talking on't; let itbe done: away, away!\\n\\necond Citiz:\\nwor, good citiz.\\n\\n Citiz:\\n accountedpoor\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02275766-8bd6-43a3-8aa5-e829a98c09d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Firs Citiz:\\nore we proceed any further, hear me speak.\\n\\nll:\\nak, speak.\\n\\n Citiz:\\nre allresolved rather to die than to famish?\\n\\nll:\\nsolved. resolved.\\n\\n Citiz:\\nrs,you know Caius Marcius is chief enemy to the people.\\n\\nll:\\nnow't, we know't.\\n\\n Citiz:\\nus killhim, and we'llhave corn a our own price.\\nIs't averdict?\\n\\nll:\\nmore talking on't; let itbe done: away, away!\\n\\necond Citiz:\\nword, good citiz.\\n\\n Citiz:\\n accounted poor\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c925340-2757-40e7-aa35-4a5d6f668677",
   "metadata": {},
   "source": [
    "## 3) Analyze our Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cfc46c0-7c13-45fe-ae7e-e6e5c5cfedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(encoding_map):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary called encoding_map and prints out the text that each token relates to\n",
    "    \"\"\"\n",
    "    encodings = list(encoding_map.items())\n",
    "    tok_text = {}\n",
    "    for (tok_1, tok_2), mapped_token in encodings:\n",
    "        text = []\n",
    "        # Append the text representation of tok_1\n",
    "        if tok_1 in tok_text:\n",
    "            text.append(tok_text[tok_1])\n",
    "        else:\n",
    "            text.append(chr(tok_1))\n",
    "        # Append the text representation of tok_2\n",
    "        if tok_2 in tok_text:\n",
    "            text.append(tok_text[tok_2])\n",
    "        else:\n",
    "            text.append(chr(tok_2))\n",
    "        # Join the characters or strings in 'text' list into a single string\n",
    "        tok_text[mapped_token] = ''.join(text)\n",
    "    \n",
    "    tok_text_list = list(tok_text.items())\n",
    "    for tok, text in tok_text_list:\n",
    "        # Replace newline characters with a visible representation\n",
    "        safe_text = text.replace('\\n', '\\\\n')\n",
    "        print(f\"Token: {tok}, Text: '{safe_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be3b9b31-e618-4a0b-a49a-81e33fc54e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 128, Text: ':\\n'\n",
      "Token: 129, Text: ' a'\n",
      "Token: 130, Text: '\\n\\n'\n",
      "Token: 131, Text: 'it'\n",
      "Token: 132, Text: 'en'\n",
      "Token: 133, Text: ' C'\n",
      "Token: 134, Text: 'iti'\n",
      "Token: 135, Text: 'itiz'\n",
      "Token: 136, Text: 'll'\n",
      "Token: 137, Text: 'rs'\n",
      "Token: 138, Text: ' Citiz'\n",
      "Token: 139, Text: '.\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "## Let's print out the text associated with our tokens that we encoded\n",
    "print_tokens(tokenizer.encmap_getter())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
