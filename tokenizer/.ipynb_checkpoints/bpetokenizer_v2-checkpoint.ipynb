{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9a3844-fb2d-456d-b317-8cba5bdb71d8",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding Tokenizer\n",
    "---\n",
    "Following Andrej Karpathy's [video](https://www.youtube.com/watch?v=zduSFxRajkE) for tokenizers, building my own BPE tokenizer. From this, will use it as a basis to train my own embedding model based on Word2Vec and analyze the results of that training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2894460-9169-435c-95b6-8020f50ca12b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1) Import Dependencies and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221d3285-ecb0-4501-8898-02a163cf44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import regex as re\n",
    "import unittest\n",
    "\n",
    "# Also defining our regex up here (from GPT-4 tokenizer)\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680c07fc-faf8-4d47-a1c6-b11f4db9a7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# First, need to import data. We'll use tiny shakespeare -- will be able to clearly see what kinds of words are commonly used in the ye olde times\n",
    "file_path = '../data/tinyshakespeare.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Print first 100 chars\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a23bec-dabd-4c81-8782-e34dd9ea7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also define unittests up here so we can call below\n",
    "class BPETokenizerTest(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Setup your regex and BPETokenizer instance here\n",
    "        self.regex = GPT4_SPLIT_PATTERN\n",
    "        self.tokenizer = BPETokenizer(self.regex)\n",
    "        self.text = data[:100000]\n",
    "\n",
    "\n",
    "    def test_encode(self):\n",
    "        # Train the tokenizer with some initial text to set up encoding_map\n",
    "        self.tokenizer.train(self.text, vocab_size=170)\n",
    "        encoded = self.tokenizer.encode(self.text)\n",
    "        # Make sure to assert that encoded is not empty and meets expected structure\n",
    "        self.assertNotEqual(len(encoded), 0, \"Encoding should not be empty.\")\n",
    "\n",
    "    def test_decode(self):\n",
    "        self.tokenizer.train(self.text, vocab_size=200)\n",
    "        encoded = self.tokenizer.encode(self.text)\n",
    "        decoded = self.tokenizer.decode(encoded)\n",
    "        self.assertEqual(decoded, self.text, \"Decoded text should match the original.\")\n",
    "\n",
    "    def test_mergepair_effectiveness(self):\n",
    "        # This will indirectly test _mergepair via the train method\n",
    "        text = \"test test test test\"\n",
    "        self.tokenizer.train(text, vocab_size=130)\n",
    "        # After training, the tokenizer should have created a token for the repeated sequence\n",
    "        # The exact token ID might vary, so we check if the length of encoding_map has increased\n",
    "        self.assertGreater(len(self.tokenizer.encoding_map), 0, \"Encoding map should have merged pairs.\")\n",
    "\n",
    "def run_tests():\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(BPETokenizerTest)\n",
    "    unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f8096918-897c-45e5-a0c9-5c5877ef266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our TrieNode and associated functions to help us speed up encoding\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token = None\n",
    "\n",
    "def add_to_trie(root, ascii_sequence, token):\n",
    "    current = root\n",
    "    for ascii_val in ascii_sequence:\n",
    "        if ascii_val not in current.children:\n",
    "            current.children[ascii_val] = TrieNode()\n",
    "        current = current.children[ascii_val]\n",
    "    current.token = token\n",
    "\n",
    "def find_longest_matching_token(root, ascii_sequence):\n",
    "    current = root\n",
    "    last_token = None\n",
    "    for ascii_val in ascii_sequence:\n",
    "        if ascii_val in current.children:\n",
    "            current = current.children[ascii_val]\n",
    "            if current.token:\n",
    "                last_token = current.token\n",
    "        else:\n",
    "            break\n",
    "    return last_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46bdd5d-0e8b-4388-826d-314bc318eb00",
   "metadata": {},
   "source": [
    "## 2) Defining our BPETokenizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91943fc-8063-4198-8740-88e844beffc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Approach\n",
    "---\n",
    "For our tokenizer, we'll create a class called BPETokenizer. Upon instantiation, it will take in a regex expression (optional). Subfunctions include:\n",
    "1. Various getters and setters\n",
    "2. _encodetext(text): Takes in raw text and returns 2-D python list of ASCII values (if regex, split by regex; else just 2-D w/ 1 element)\n",
    "3. _mergepair(encodedtext, token_pair): helper function to take in your encoded text and a tuple (pair, token) that replaces the pair with the desired token\n",
    "4. train(text, vocab_size): Function to iteratively convert most frequent byte-pairs to a new token. Saves encoding map to class\n",
    "5. _optimize_mappings: Function that creates our TrieTree (for fast encoding) and an optimized decoding mapping (for fast decoding)\n",
    "6. encode(text): Converts text to tokens; returns as 1-d python list of tokens\n",
    "7. decode(tokens): Converts tokens to text; returns string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7c7c1aab-d47b-4929-8fa1-94ac5a47164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "\n",
    "    def __init__(self, regex=None):\n",
    "        self.encoding_map = {}\n",
    "        self.decoding_map = {}\n",
    "        self.regex = regex\n",
    "\n",
    "    def regex_setter(self, re):\n",
    "        \"\"\" Simple setter function for regex \"\"\"\n",
    "        self.regex = re\n",
    "\n",
    "    def encmap_getter(self):\n",
    "        \"\"\" Simple getter to return our encoding map \"\"\"\n",
    "        return self.encoding_map\n",
    "        \n",
    "    def decmap_getter(self):\n",
    "        \"\"\" Simple getter to return our decoding map \"\"\"\n",
    "        return self.decoding_map\n",
    "\n",
    "    def _encodetext(self, text):\n",
    "        \"\"\"\n",
    "            Takes in raw text and returns 2-D python array of ASCII values (e.g., [[70, 105, 114, 115, 116], [32, 67,...]])\n",
    "            Replaces non-ASCII with '?'\n",
    "        \"\"\"\n",
    "        if self.regex == None:\n",
    "            text = [text]\n",
    "        else:\n",
    "            text = re.findall(self.regex, text)     # Converts text to ['First', ' Citizen', ':\\n', 'Before', ' we', ' proceed',...]\n",
    "        return list( list(t.encode(\"ascii\", errors=\"replace\")) for t in text )\n",
    "\n",
    "    def _mergepair(self, tokens, pair_tok):\n",
    "        \"\"\"\n",
    "            Takes in 2-D python list of lists of tokens and iterates through, replacing 'tok' where 'pair' exists\n",
    "        \"\"\"\n",
    "        pair, tok = pair_tok\n",
    "        merged_tokens = []\n",
    "        for block in tokens:\n",
    "            merged_block = []\n",
    "            skip_next_idx = False\n",
    "            if (len(block) < 2):\n",
    "                merged_block = block           # Simply keep our block the same if not 2+ elements\n",
    "            else:\n",
    "                for idx in range(len(block) - 1):\n",
    "                    if skip_next_idx:\n",
    "                        skip_next_idx = False\n",
    "                        if idx == len(block) - 2:\n",
    "                            merged_block.append(block[idx + 1])\n",
    "                    elif block[idx] == pair[0] and block[idx + 1] == pair[1]:\n",
    "                        merged_block.append(tok)  # Merge pair into 'tok'\n",
    "                        skip_next_idx = True  # Skip the next index as it's already merged\n",
    "                        if idx == len(block) - 2:  # Check if at the end after merging\n",
    "                            break  # No need to append anything else, as the pair was the last two elements\n",
    "                    else:\n",
    "                        merged_block.append(block[idx])\n",
    "                        if idx == len(block) - 2:  # Handle the last element if it's not part of a pair\n",
    "                            merged_block.append(block[idx + 1])\n",
    "\n",
    "            merged_tokens.append(merged_block)\n",
    "        return merged_tokens\n",
    "\n",
    "    def train(self, text, vocab_size):\n",
    "        \"\"\"\n",
    "            Takes in raw text and a desired length for max vocabulary size\n",
    "            Upon completion, sets encoder_map and decoder_map to the BPE mappings (forward, backward resp.) \n",
    "        \"\"\"\n",
    "        encoded_text = self._encodetext(text)\n",
    "        encoding_map = {}                 # Create python dictionary of merges\n",
    "        num_merges = vocab_size - 128     # Number of iterations of BPE\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            bytepair_count = {}\n",
    "            for block in encoded_text:\n",
    "                if len(block) > 1:\n",
    "                    for idx in range(len(block)-1):\n",
    "                        pair = (block[idx], block[idx+1])\n",
    "                        count = bytepair_count.get(pair, 0)\n",
    "                        bytepair_count[pair] = count+1\n",
    "            \n",
    "            # Once done iterating through all the ascii values, sort and assign most freq bytepair to new token\n",
    "            freq_pair = max(bytepair_count, key=bytepair_count.get)\n",
    "            new_token = 128 + i\n",
    "            encoding_map[freq_pair] = new_token\n",
    "            encoded_text = self._mergepair(encoded_text, (freq_pair, new_token))\n",
    "\n",
    "        self.encoding_map = encoding_map\n",
    "        self.decoding_map = {value: key for key, value in encoding_map.items()}\n",
    "        self._optimize_mapping()   # Run our mapping optimizer so we can more efficiently encode / decode in the future\n",
    "\n",
    "    def _optimize_mapping(self):\n",
    "        \"\"\"\n",
    "            Creates self.optimized_encoding_map and self.optimized_decoding_map from their primitive parent mappings.\n",
    "            These optimized mappings allow us to compute these optimized mappings once and save time on calling encode / decode:\n",
    "                encode(): finds longest token that matches the initial encodings and replaces; prevents iterative process\n",
    "                decode(): goes directly from token to full length of chars (rather than taking iterative steps)\n",
    "        \"\"\"\n",
    "        optimized_decoding_map = {i: [i] for i in range(128)}\n",
    "        decoding_map_list = list(self.decoding_map.items())\n",
    "        for tok, pair in decoding_map_list:\n",
    "            expanded_pair = [pair[0], pair[1]]\n",
    "            while max(expanded_pair)>127:\n",
    "                for i, pair_token in enumerate(expanded_pair):\n",
    "                    expanded_pair[i:i+1] = optimized_decoding_map[pair_token]\n",
    "            optimized_decoding_map[tok] = expanded_pair\n",
    "        \n",
    "        self.optimized_decoding_map = optimized_decoding_map\n",
    "        self.optimized_encoding_map = {tuple(value): key for key, value in optimized_decoding_map.items()}\n",
    "        # Defining our TrieNode as well to speed up encoding\n",
    "        self.trie_root = TrieNode()\n",
    "        for ascii_sequence, token in self.optimized_encoding_map.items():\n",
    "            add_to_trie(self.trie_root, ascii_sequence, token)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "            Takes in raw text and returns tokenized text (1-D array)\n",
    "        \"\"\"\n",
    "        encoded_text = self._encodetext(text)\n",
    "        tokenized_text = []\n",
    "        for block in encoded_text:\n",
    "            next_idx = 0\n",
    "            while next_idx < len(block):\n",
    "                token = find_longest_matching_token(self.trie_root, block[next_idx:])\n",
    "                next_idx = next_idx + len(self.optimized_decoding_map[token])\n",
    "                tokenized_text.append(token)\n",
    "        return tokenized_text\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"\n",
    "            Takes in tokenized text (1-D array) and returns raw text\n",
    "        \"\"\"\n",
    "        token_text = []\n",
    "        for i in tokens:\n",
    "            token_text.extend(self.optimized_decoding_map[i])\n",
    "        decoded_text = ''.join(chr(value) for value in token_text)\n",
    "        return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dd3f86f3-090b-4017-96fa-d5266b51c957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_decode (__main__.BPETokenizerTest.test_decode) ... ok\n",
      "test_encode (__main__.BPETokenizerTest.test_encode) ... ok\n",
      "test_mergepair_effectiveness (__main__.BPETokenizerTest.test_mergepair_effectiveness) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 3.481s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Running our tests \n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7889d758-d1b0-4e9f-b477-d3c9cef816c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define our data we'll be trianing on and tokenize it\n",
    "text_data = data[:2000]\n",
    "tokenizer = BPETokenizer(GPT4_SPLIT_PATTERN)\n",
    "tokenizer.train(text_data, 200)\n",
    "tokenized_text = tokenizer.encode(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f102f8bd-294b-4dfe-9a4f-833ce55e9ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talkin\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_text[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c925340-2757-40e7-aa35-4a5d6f668677",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3) Analyze our Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4cfc46c0-7c13-45fe-ae7e-e6e5c5cfedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(encoding_map):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary called encoding_map and prints out the text that each token relates to\n",
    "    \"\"\"\n",
    "    encodings = list(encoding_map.items())\n",
    "    tok_text = {}\n",
    "    for (tok_1, tok_2), mapped_token in encodings:\n",
    "        text = []\n",
    "        # Append the text representation of tok_1\n",
    "        if tok_1 in tok_text:\n",
    "            text.append(tok_text[tok_1])\n",
    "        else:\n",
    "            text.append(chr(tok_1))\n",
    "        # Append the text representation of tok_2\n",
    "        if tok_2 in tok_text:\n",
    "            text.append(tok_text[tok_2])\n",
    "        else:\n",
    "            text.append(chr(tok_2))\n",
    "        # Join the characters or strings in 'text' list into a single string\n",
    "        tok_text[mapped_token] = ''.join(text)\n",
    "    \n",
    "    tok_text_list = list(tok_text.items())\n",
    "    for tok, text in tok_text_list:\n",
    "        # Replace newline characters with a visible representation\n",
    "        safe_text = text.replace('\\n', '\\\\n')\n",
    "        print(f\"Token: {tok}, Text: '{safe_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "be3b9b31-e618-4a0b-a49a-81e33fc54e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 128, Text: ' t'\n",
      "Token: 129, Text: 'he'\n",
      "Token: 130, Text: 'it'\n",
      "Token: 131, Text: 'en'\n",
      "Token: 132, Text: 'ou'\n",
      "Token: 133, Text: ' a'\n",
      "Token: 134, Text: ' w'\n",
      "Token: 135, Text: 're'\n",
      "Token: 136, Text: ':\\n'\n",
      "Token: 137, Text: '\\n\\n'\n",
      "Token: 138, Text: 'st'\n",
      "Token: 139, Text: ' C'\n",
      "Token: 140, Text: 'is'\n",
      "Token: 141, Text: 'on'\n",
      "Token: 142, Text: 'iti'\n",
      "Token: 143, Text: ' the'\n",
      "Token: 144, Text: ' h'\n",
      "Token: 145, Text: 'itiz'\n",
      "Token: 146, Text: 'itizen'\n",
      "Token: 147, Text: 'at'\n",
      "Token: 148, Text: 'ir'\n",
      "Token: 149, Text: ' to'\n",
      "Token: 150, Text: ' c'\n",
      "Token: 151, Text: 'in'\n",
      "Token: 152, Text: ' Citizen'\n",
      "Token: 153, Text: ' p'\n",
      "Token: 154, Text: ' s'\n",
      "Token: 155, Text: '.\\n\\n'\n",
      "Token: 156, Text: 'irst'\n",
      "Token: 157, Text: ' he'\n",
      "Token: 158, Text: 'll'\n",
      "Token: 159, Text: 'us'\n",
      "Token: 160, Text: 'or'\n",
      "Token: 161, Text: ' b'\n",
      "Token: 162, Text: ' f'\n",
      "Token: 163, Text: 'First'\n",
      "Token: 164, Text: 've'\n",
      "Token: 165, Text: 'no'\n",
      "Token: 166, Text: ' we'\n",
      "Token: 167, Text: ' m'\n",
      "Token: 168, Text: ' th'\n",
      "Token: 169, Text: 'ic'\n",
      "Token: 170, Text: 'ay'\n",
      "Token: 171, Text: 'ar'\n",
      "Token: 172, Text: 'pe'\n",
      "Token: 173, Text: ' d'\n",
      "Token: 174, Text: 'an'\n",
      "Token: 175, Text: 'es'\n",
      "Token: 176, Text: ' re'\n",
      "Token: 177, Text: ' o'\n",
      "Token: 178, Text: ' be'\n",
      "Token: 179, Text: 'con'\n",
      "Token: 180, Text: ' g'\n",
      "Token: 181, Text: ' pr'\n",
      "Token: 182, Text: 'ak'\n",
      "Token: 183, Text: 'All'\n",
      "Token: 184, Text: ' y'\n",
      "Token: 185, Text: ' us'\n",
      "Token: 186, Text: 'er'\n",
      "Token: 187, Text: ' for'\n",
      "Token: 188, Text: ' in'\n",
      "Token: 189, Text: 'ce'\n",
      "Token: 190, Text: ' an'\n",
      "Token: 191, Text: 'peak'\n",
      "Token: 192, Text: 'ol'\n",
      "Token: 193, Text: 'ie'\n",
      "Token: 194, Text: ' you'\n",
      "Token: 195, Text: ' k'\n",
      "Token: 196, Text: 'le'\n",
      "Token: 197, Text: ' hi'\n",
      "Token: 198, Text: ' him'\n",
      "Token: 199, Text: 'Se'\n"
     ]
    }
   ],
   "source": [
    "# Let's print out the text associated with our tokens that we encoded\n",
    "# print_tokens(tokenizer.encmap_getter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5f32960e-f083-4bdf-906f-9020b0e0aaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original string:     2000\n",
      "Length of tokenized string:    1132\n",
      "Compression (% of original):   56.60%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can check the compression rate\n",
    "print(f\"Length of original string:     {len(text_data)}\")\n",
    "print(f\"Length of tokenized string:    {len(tokenized_text)}\")\n",
    "print(f\"Compression (% of original):   {((len(tokenized_text) / len(text_data))*100):.2f}%\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Let's make sure the text is properly decodable as well\n",
    "# print(tokenizer.decode(tokenized_text[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d635fde7-9793-4773-87ec-66d2784e42ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4) Optimizing our Code and Analyzing Runtime Performance\n",
    "---\n",
    "Able to successfully optimize the code to run significantly (>100x) faster on encoding runs. Detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b831a-0264-464f-8734-6d1b109feeed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Define Dependencies and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7367fad5-c423-4b8d-959a-1fb001352515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to analyze runtime performance \n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import time as pytime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from matplotlib.patches import Patch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74027226-b419-4ab1-b780-dc73c6938093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_vs_configurations(run_times, metric_label=\"Metric\", varying_param_label=\"Parameter\"):\n",
    "    \"\"\"\n",
    "    Plots times versus configurations for a given metric, with configurations represented as tuples.\n",
    "    \n",
    "    Parameters:\n",
    "        run_times (dict): A dictionary where keys are tuples representing configurations (e.g., (metric_size, varying_param))\n",
    "                          and values are times.\n",
    "        metric_label (str): The label for the x-axis, indicating the metric being measured (default is 'Metric').\n",
    "        varying_param_label (str): The description for varying parameters, used in legend labeling (default is 'Parameter').\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    \n",
    "    # Extract metric sizes and varying parameters from dictionary keys\n",
    "    metric_sizes = sorted(set(key[0] for key in run_times.keys()))\n",
    "    varying_params = sorted(set(key[1] for key in run_times.keys()))\n",
    "    \n",
    "    for param in varying_params:\n",
    "        times = [run_times.get((metric, param), None) for metric in metric_sizes]\n",
    "        plt.plot(metric_sizes, times, marker='o', label=f\"{varying_param_label} {param}\")\n",
    "    \n",
    "    plt.title(f\"Time vs. {metric_label}\")\n",
    "    plt.xlabel(metric_label)\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99c3168a-0624-4c70-907e-b43ac6d575a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_polynomial_model_from_dict(data_dict, degree=2):\n",
    "    \"\"\"\n",
    "    Trains a polynomial regression model from a dictionary where keys are tuples representing features\n",
    "    and values are targets.\n",
    "    \n",
    "    Parameters:\n",
    "        data_dict (dict): A dictionary with keys as tuples representing feature values and values as target values.\n",
    "        degree (int): The degree of the polynomial features.\n",
    "        \n",
    "    Returns:\n",
    "        (X, y): Tuple of our data used in our training\n",
    "        poly_model (LinearRegression): The trained polynomial regression model.\n",
    "        poly (PolynomialFeatures): The polynomial feature transformer used for the model.\n",
    "    \"\"\"\n",
    "    # Extract features (X) and targets (y) from the dictionary\n",
    "    X = np.array(list(data_dict.keys()))\n",
    "    y = np.array(list(data_dict.values()))\n",
    "\n",
    "    # Transform your features into polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    # Fit the linear regression model on polynomial features\n",
    "    poly_model = LinearRegression()\n",
    "    poly_model.fit(X_poly, y)\n",
    "    \n",
    "    return (X, y), poly_model, poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73c3b1d6-8ed3-4dff-ba9a-5888436faefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_over_optimizations_observed(results_list):\n",
    "    \"\"\"\n",
    "    Prints the average performance improvement across all configurations for each run,\n",
    "    compared to the initial run.\n",
    "    \"\"\"\n",
    "    initial_time = sum(results_list[0].values())\n",
    "    for i, run_data in enumerate(results_list[1:], 1):\n",
    "        current_time = sum(run_data.values())\n",
    "        percentage = (current_time / initial_time) * 100\n",
    "        improvement_factor = 100 / percentage\n",
    "        print(f\"Optimization {i}: {percentage:.2f}% of initial time; ({improvement_factor:.2f}x speed improvement).\")\n",
    "\n",
    "def performance_over_optimizations_predictions(number_list):\n",
    "    \"\"\"Prints the improvement factor over the initial value for a series of optimization runs.\"\"\"\n",
    "    if not number_list:\n",
    "        print(\"The list is empty.\")\n",
    "        return\n",
    "\n",
    "    initial = number_list[0]\n",
    "    for i, value in enumerate(number_list[1:], 1):\n",
    "        percentage = (value / initial) * 100\n",
    "        improvement_factor = initial / value\n",
    "        print(f\"Optimization {i}: {percentage:.2f}% of initial time; ({improvement_factor:.2f}x speed improvement).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6afac6b7-7f08-4d81-bf86-3d0dbe4513f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polynomial_regression_surface(X, y, poly, poly_model, feature_labels=['Feature 1', 'Feature 2'], target_label='Target'):\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Scatter plot of original data points\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], y, color='blue', label='Actual Data')\n",
    "\n",
    "    # Generate mesh for the surface\n",
    "    x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 20)\n",
    "    x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 20)\n",
    "    x1_mesh, x2_mesh = np.meshgrid(x1_range, x2_range)\n",
    "    mesh_flat = np.c_[x1_mesh.ravel(), x2_mesh.ravel()]\n",
    "    mesh_poly = poly.transform(mesh_flat)  # Transform to polynomial features\n",
    "    y_mesh = poly_model.predict(mesh_poly)  # Predict using the polynomial model\n",
    "    y_mesh = y_mesh.reshape(x1_mesh.shape)\n",
    "\n",
    "    # Plot the surface\n",
    "    surface = ax.plot_surface(x1_mesh, x2_mesh, y_mesh, color='orange', alpha=0.5, edgecolor='none')\n",
    "\n",
    "    ax.set_xlabel(feature_labels[0])\n",
    "    ax.set_ylabel(feature_labels[1])\n",
    "    ax.set_zlabel(target_label)\n",
    "    ax.set_title(f'3D Plot of {feature_labels[0]}, {feature_labels[1]} vs. {target_label} with Polynomial Regression Surface')\n",
    "\n",
    "    # Manually define legend\n",
    "    scatter_legend = Patch(color='blue', label='Actual Data')\n",
    "    surface_legend = Patch(color='orange', label='Polynomial Regression Model')\n",
    "    plt.legend(handles=[scatter_legend, surface_legend])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63e38852-2544-493f-816a-6d00a9bd9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_run(datasizes, vocabsizes, printprog = True):\n",
    "    \"\"\" Function that completes a training run for multiple datasizes and vocabsizes(lists of params). Returns the data for each run as a dict {(tuple of params (D, V)): Time} \"\"\"\n",
    "    dict_results = {}\n",
    "    for d in datasizes:\n",
    "        runtime_testdata = data[:d]\n",
    "        for v in vocabsizes:\n",
    "            rt_tokenizer = BPETokenizer(GPT4_SPLIT_PATTERN)\n",
    "            \n",
    "            start_time = pytime.time()  # Start timer\n",
    "            rt_tokenizer.train(runtime_testdata, v)\n",
    "            end_time = pytime.time()  # End timer\n",
    "            \n",
    "            run_time = end_time - start_time\n",
    "            dict_results[(d, v)] = run_time  # Save the time with (d, v) as the key\n",
    "            if printprog:\n",
    "                print(f\"D: {d}, V: {v}; Time: {run_time:.2f}s\")    \n",
    "    \n",
    "    return dict_results\n",
    "\n",
    "def encoding_run(vocabsizes, encodinglengths, traindata, printprog = True):\n",
    "    \"\"\" Function that completes an encoding run for multiple vocabsizes and encodinglengths (lists of params). Returns the data for each run as a dict {(tuple of params (V, L)): Time} \"\"\"\n",
    "    dict_results = {}\n",
    "    for v in vocabsizes:\n",
    "        rt_tokenizer = BPETokenizer(GPT4_SPLIT_PATTERN)\n",
    "        rt_tokenizer.train(traindata, v)\n",
    "        \n",
    "        for l in encodinglengths:\n",
    "            start_time = pytime.time()  # Start timer\n",
    "            rt_tokenizer.encode(data[:l])\n",
    "            end_time = pytime.time()  # End timer\n",
    "            run_time = end_time - start_time\n",
    "            dict_results[(v, l)] = run_time  # Save the time with (l, v) as the key\n",
    "            print(f\"V: {v}, L: {l}; Time: {run_time:.2f}s\")\n",
    "\n",
    "    return dict_results\n",
    "\n",
    "def pred_data(datapoint, polyfunc, polymodel):\n",
    "    \"\"\" Function that prints and returns the predicted time for the input data based on the trained polynomial fit \"\"\"\n",
    "    datapoint_poly = polyfunc.transform(datapoint)\n",
    "    prediction = polymodel.predict(datapoint_poly)[0]\n",
    "    print(f\"Predicted Time: {prediction:.2f} seconds\")\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cad355-48ff-4c1e-9d4c-61d8979e5e7d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Progress in Optimization\n",
    "---\n",
    "Keeping a log of key changes made during each step:\n",
    "1. Initial run using our working implementation (un-optimized)\n",
    "2. Updated the mergepair function to be a bit more efficient (small edits)\n",
    "3. Implemented the Trie Node encoding function to speed up encoding. Also implemented fast decoding (compute token - chars for all tokens so just one pass through lets you decode).\n",
    "\n",
    "*Results below, but note that #3 actually increases training time because we now have an '_optimize_mapping' function that sets up our TrieNode / optimized decoding map.*\n",
    "\n",
    "\n",
    "**Improvement for Training Runs**  \n",
    "\\------------------------------------------------  \n",
    "\n",
    "Performance Over Observed Data (Avg)  \n",
    "\\================================================  \n",
    "Optimization 1: 91.80% of initial time; (1.09x speed improvement).  \n",
    "Optimization 2: 107.38% of initial time; (0.93x speed improvement).  \n",
    "\n",
    "Performance Over Predicted Time for Large Run  \n",
    "\\================================================  \n",
    "Optimization 1: 89.64% of initial time; (1.12x speed improvement).  \n",
    "Optimization 2: 103.56% of initial time; (0.97x speed improvement).  \n",
    "\n",
    "\n",
    "**Improvement for Encoding Runs**  \n",
    "\\------------------------------------------------  \n",
    "\n",
    "Performance Over Observed Data (Avg)  \n",
    "\\================================================  \n",
    "Optimization 1: 89.11% of initial time; (1.12x speed improvement).  \n",
    "Optimization 2: 0.97% of initial time; (102.62x speed improvement).  \n",
    "\n",
    "Performance Over Predicted Time for Large Run  \n",
    "\\================================================  \n",
    "Optimization 1: 91.48% of initial time; (1.09x speed improvement).  \n",
    "Optimization 2: 0.44% of initial time; (226.54x speed improvement).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1422a846-7f95-4fcc-806e-f648f917c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first define our existing performance\n",
    "# On our training run\n",
    "trainingrun_testdata = np.array([[len(data), 2048]])  # (Data_size, vocab_size)\n",
    "trainingrun_predlist     = []\n",
    "trainingrun_resultslist  = []\n",
    "\n",
    "# On our encoding runs\n",
    "encodingrun_testdata = np.array([[2048, 1000000]])  # (Vocab_size, Encoding_length)\n",
    "encodingrun_predlist     = []\n",
    "encodingrun_resultslist  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "854e6495-9214-4ce2-9287-6ddf7d9c8b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: 10000, V: 200; Time: 0.17s\n",
      "D: 10000, V: 300; Time: 0.29s\n",
      "D: 10000, V: 500; Time: 0.49s\n",
      "D: 10000, V: 1000; Time: 0.82s\n",
      "D: 30000, V: 200; Time: 0.54s\n",
      "D: 30000, V: 300; Time: 1.21s\n",
      "D: 30000, V: 500; Time: 1.97s\n",
      "D: 30000, V: 1000; Time: 3.54s\n",
      "D: 100000, V: 200; Time: 1.94s\n",
      "D: 100000, V: 300; Time: 4.28s\n",
      "D: 100000, V: 500; Time: 7.65s\n",
      "D: 100000, V: 1000; Time: 14.75s\n",
      "D: 300000, V: 200; Time: 6.80s\n",
      "D: 300000, V: 300; Time: 13.78s\n",
      "D: 300000, V: 500; Time: 25.13s\n",
      "D: 300000, V: 1000; Time: 46.74s\n",
      "Predicted Time: 373.52 seconds\n",
      "V: 200, L: 10000; Time: 0.00s\n",
      "V: 200, L: 30000; Time: 0.02s\n",
      "V: 200, L: 100000; Time: 0.06s\n",
      "V: 200, L: 300000; Time: 0.15s\n",
      "V: 300, L: 10000; Time: 0.00s\n",
      "V: 300, L: 30000; Time: 0.02s\n",
      "V: 300, L: 100000; Time: 0.03s\n",
      "V: 300, L: 300000; Time: 0.16s\n",
      "V: 500, L: 10000; Time: 0.00s\n",
      "V: 500, L: 30000; Time: 0.02s\n",
      "V: 500, L: 100000; Time: 0.03s\n",
      "V: 500, L: 300000; Time: 0.16s\n",
      "V: 1000, L: 10000; Time: 0.05s\n",
      "V: 1000, L: 30000; Time: 0.02s\n",
      "V: 1000, L: 100000; Time: 0.02s\n",
      "V: 1000, L: 300000; Time: 0.14s\n",
      "Predicted Time: 1.13 seconds\n"
     ]
    }
   ],
   "source": [
    "# After updating our code, we can run this to append our performance to our prior list\n",
    "train_data_sizes      = [10000, 30000, 100000, 300000]\n",
    "train_vocab_sizes     = [200, 300, 500, 1000]\n",
    "\n",
    "trainingrun_results = training_run(train_data_sizes, train_vocab_sizes, printprog = True)\n",
    "trainingrun_resultslist.append(trainingrun_results)\n",
    "train_data, train_polymodel, train_poly = train_polynomial_model_from_dict(trainingrun_results, degree=2)\n",
    "train_pred = pred_data(trainingrun_testdata, train_poly, train_polymodel)\n",
    "trainingrun_predlist.append(train_pred)\n",
    "\n",
    "# And for our encoding run\n",
    "encoding_traindata   = data[:10000]\n",
    "encoding_vocab_sizes = [200, 300, 500, 1000]\n",
    "encoding_lengths     = [10000, 30000, 100000, 300000]\n",
    "\n",
    "encodingrun_results = encoding_run(encoding_vocab_sizes, encoding_lengths, encoding_traindata, printprog = True)\n",
    "encodingrun_resultslist.append(encodingrun_results)\n",
    "encoding_data, encoding_polymodel, encoding_poly = train_polynomial_model_from_dict(encodingrun_results, degree=2)\n",
    "encoding_pred = pred_data(encodingrun_testdata, encoding_poly, encoding_polymodel)\n",
    "encodingrun_predlist.append(encoding_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b4afa653-9c80-490d-9af4-16791d33c36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement for Training Runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Performance Over Observed Data (Avg)\n",
      "================================================\n",
      "Optimization 1: 91.80% of initial time; (1.09x speed improvement).\n",
      "Optimization 2: 107.38% of initial time; (0.93x speed improvement).\n",
      "\n",
      "Performance Over Predicted Time for Large Run\n",
      "================================================\n",
      "Optimization 1: 89.64% of initial time; (1.12x speed improvement).\n",
      "Optimization 2: 103.56% of initial time; (0.97x speed improvement).\n",
      "\n",
      "\n",
      "\n",
      "Improvement for Encoding Runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Performance Over Observed Data (Avg)\n",
      "================================================\n",
      "Optimization 1: 89.11% of initial time; (1.12x speed improvement).\n",
      "Optimization 2: 0.97% of initial time; (102.62x speed improvement).\n",
      "\n",
      "Performance Over Predicted Time for Large Run\n",
      "===================================================\n",
      "Optimization 1: 91.48% of initial time; (1.09x speed improvement).\n",
      "Optimization 2: 0.44% of initial time; (226.54x speed improvement).\n"
     ]
    }
   ],
   "source": [
    "# Let's see how our performance has been improving for training runs\n",
    "print(\"Improvement for Training Runs\\n---------------------------------------------------\")\n",
    "print(\"\\nPerformance Over Observed Data (Avg)\\n================================================\")\n",
    "training_run_performance = performance_over_optimizations_observed(trainingrun_resultslist)\n",
    "print(\"\\nPerformance Over Predicted Time for Large Run\\n================================================\")\n",
    "performance_over_optimizations_predictions(trainingrun_predlist)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# And for encoding runs\n",
    "print(\"Improvement for Encoding Runs\\n---------------------------------------------------\")\n",
    "print(\"\\nPerformance Over Observed Data (Avg)\\n================================================\")\n",
    "training_run_performance = performance_over_optimizations_observed(encodingrun_resultslist)\n",
    "print(\"\\nPerformance Over Predicted Time for Large Run\\n===================================================\")\n",
    "performance_over_optimizations_predictions(encodingrun_predlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4cd3e965-7f35-49fe-9818-ba5dfd80eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to optimization_results\\2024-03-06_18-40-22.pkl\n"
     ]
    }
   ],
   "source": [
    "# Optional code that will save our data to a folder\n",
    "def save_optimization_results():\n",
    "    filename = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + '.pkl'\n",
    "    filepath = os.path.join('optimization_results', filename)\n",
    "    data_to_save = {\n",
    "        'trainingrun_testdata': trainingrun_testdata,\n",
    "        'trainingrun_predlist': trainingrun_predlist,\n",
    "        'trainingrun_resultslist': trainingrun_resultslist,\n",
    "        'encodingrun_testdata': encodingrun_testdata,\n",
    "        'encodingrun_predlist': encodingrun_predlist,\n",
    "        'encodingrun_resultslist': encodingrun_resultslist\n",
    "    }\n",
    " \n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(data_to_save, file)\n",
    "    \n",
    "    print(f\"Data saved to {filepath}\")\n",
    "\n",
    "# save_optimization_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d71613-3e50-43d1-aefc-a8fac2396724",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Analyze the Time for Our Training Runs\n",
    "---\n",
    "Scales with a slight parabolic trend -- especially as datasize grows beyond 100k chars and vocab size grows beyond 500.  \n",
    "**Post-Optimization**: There are some optimizations we could make to run this quicker -- notably around parallelization and more efficiently counting the occurances of our pairs -- but because we only call train once and these are not likely to lead to huge speed improvements (except at large scale that is beyond what my laptop is capable of doing), this doesn't seem worth the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae32c7-060f-4401-91de-bb4d479f80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the time required to run based on size of dataset and size of vocabulary and see how they grow\n",
    "# train_run_times = training_run(data_sizes, vocab_sizes, printprog = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f0ad4-bc4d-4339-8bc7-6187dee55eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can plot our Training Time vs. Data Size \n",
    "plot_time_vs_configurations(trainingrun_resultslist[-1], metric_label=\"Data Size\", varying_param_label=\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a502e-6476-457b-83ec-f9e376454e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our polynomial fit\n",
    "train_data, train_polymodel, train_poly = train_polynomial_model_from_dict(trainingrun_resultslist[-1], degree=2)\n",
    "\n",
    "# Predict based on our data\n",
    "train_pred = pred_data(trainingrun_testdata, train_poly, train_polymodel)\n",
    "\n",
    "# Visualize our graph\n",
    "plot_polynomial_regression_surface(train_data[0], train_data[1], train_poly, train_polymodel, feature_labels=['Data Size', 'Vocab Size'], target_label='Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb09ed0-cea2-418c-a1f4-0d30b2ca9704",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Analyzing Runtime of Encoding\n",
    "---\n",
    "We can see that the runtime of encoding grows with slight parabolic trajectory in our base implementation. We'll work to reduce this above in the optimization section.  \n",
    "**Post-Optimization:** We are able to reduce this to trivial time by using a TrieNode. For example, encoding 1M chars using a vocab size of 2048 previously would have taken ~4 minutes, now it runs in around 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74d2d4-72a4-48b0-8517-494ddb51ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the time required to run based on size of dataset and size of vocabulary and see how they grow\n",
    "# encodingrun_results = encoding_run(encoding_vocab_sizes, encoding_lengths, printprog = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941c825-579d-4e1b-8ae2-790d3d4253a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can plot our Training Time vs. Encoding Length \n",
    "plot_time_vs_configurations(encodingrun_resultslist[-1], metric_label=\"Encoding Length\", varying_param_label=\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8fd6a-9e0b-44e2-96dd-13cb2585ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our polynomial fit\n",
    "encoding_data, encoding_polymodel, encoding_poly = train_polynomial_model_from_dict(encodingrun_resultslist[-1], degree=2)\n",
    "\n",
    "# Predict based on our data\n",
    "encoding_pred = pred_data(encodingrun_testdata, encoding_poly, encoding_polymodel)\n",
    "\n",
    "# Visualize our graph\n",
    "plot_polynomial_regression_surface(encoding_data[0], encoding_data[1], encoding_poly, encoding_polymodel, feature_labels=['Vocab Size', 'Encoding Length'], target_label='Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31d417-f27c-495b-a41b-4265c5bc2f87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Other Runtime Analysis Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247ff6b-8816-4631-97a9-4d4bd7b4cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our code that we want to test runtime on\n",
    "runtime_testdata = data[:10000]\n",
    "rt_tokenizer = BPETokenizer(GPT4_SPLIT_PATTERN)\n",
    "\n",
    "def profile_your_function():\n",
    "    rt_tokenizer.train(runtime_testdata, 500)\n",
    "    # rt_tokenized_text = rt_tokenizer.encode(runtime_testdata)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a13913-a297-432d-b1a6-fa8b86e28f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Profile object\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()  # Start collecting profiling data\n",
    "\n",
    "# Call the function or part of the code you want to profile\n",
    "profile_your_function()\n",
    "\n",
    "pr.disable()  # Stop collecting profiling data\n",
    "\n",
    "# Use io.StringIO to capture the profiling output\n",
    "s = io.StringIO()\n",
    "# Sort the statistics by cumulative time spent and print the top parts\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')\n",
    "ps.print_stats()\n",
    "\n",
    "# Print the profiling result\n",
    "print(s.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
