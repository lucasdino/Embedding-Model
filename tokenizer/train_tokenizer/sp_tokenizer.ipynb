{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ab6d92-0921-4fe1-9c77-61cdd1f3a840",
   "metadata": {},
   "source": [
    "## Using SentencePiece to Train a Tokenizer on a mini-batch of data from enwikisource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "747a99da-d86f-44d0-b572-82d57c0eae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import sentencepiece as sp\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cd1373-5c93-45d1-ae61-ce3213f77163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to data and set var for model prefix\n",
    "input_file = '../../data/enwiki_20240320_minibatch.txt'\n",
    "model_prefix = '../models/sptokenizer_16384'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9b7cfc6-12eb-4da8-867f-c25f5ec61cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocabulary have been generated: ../models/sptokenizer_16384.model and ../models/sptokenizer_16384.vocab\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "sp.SentencePieceTrainer.train(input=input_file,\n",
    "                               model_prefix=model_prefix,\n",
    "                               vocab_size=16384,\n",
    "                               character_coverage=1.0,\n",
    "                               model_type='bpe')\n",
    "\n",
    "print(f'Model and vocabulary have been generated: {model_prefix}.model and {model_prefix}.vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4880c-4f02-4c0f-963b-e43270910326",
   "metadata": {},
   "source": [
    "## Compute distribution of tokens across our training data\n",
    "---\n",
    "Need to get a token count so that we can conduct effective negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc8e3abf-b948-4274-9f30-09e48e5cbc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First load our model\n",
    "model_path = \"../models/sptokenizer_16384.model\"\n",
    "model = sp.SentencePieceProcessor()\n",
    "model.load(model_path)\n",
    "\n",
    "# Testing our model\n",
    "model.decode(model.encode(\"Hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40f77060-590f-4056-8321-5350264f441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now - we need to iterate through our data and get a count for the number of times each token is seen\n",
    "def count_tokens(data_file, spm_model):\n",
    "    token_counts = Counter()\n",
    "    with open(data_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                token_counts.update(spm_model.encode(line))\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "995fd486-33b4-4fc6-bdce-91189a0b27b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID     Count    Word\n",
      "------------------------\n",
      "16210: 215732 - \",\"\n",
      "    6: 205933 - \"the\"\n",
      "   16: 129767 - \"of\"\n",
      "   21: 110446 - \"and\"\n",
      "16213: 109705 - \".\"\n",
      "   32:  80859 - \"to\"\n",
      "   33:  55222 - \"in\"\n",
      "    5:  47629 - \"a\"\n",
      "   58:  40097 - \"that\"\n",
      "   74:  28892 - \"is\"\n",
      "   48:  28110 - \"I\"\n",
      "   44:  27899 - \"be\"\n",
      "   65:  27017 - \"for\"\n",
      "16220:  25077 - \";\"\n",
      "   83:  22347 - \"it\"\n",
      "   87:  20951 - \"with\"\n",
      "   93:  20755 - \"not\"\n",
      "   89:  19772 - \"as\"\n",
      "16224:  17964 - \":\"\n",
      "  111:  17868 - \"by\"\n",
      "  110:  17845 - \"his\"\n",
      "   63:  17648 - \"he\"\n",
      "   98:  17068 - \"or\"\n",
      "  119:  16701 - \"shall\"\n",
      "  121:  15817 - \"was\"\n",
      "  128:  15696 - \"which\"\n",
      "16197:  15200 - \"s\"\n",
      "  133:  15194 - \"have\"\n",
      "  130:  14607 - \"all\"\n",
      "  142:  14390 - \"they\"\n",
      "  153:  13490 - \"And\"\n",
      "  147:  13462 - \"are\"\n",
      "  101:  13419 - \"The\"\n",
      "   88:  13365 - \"on\"\n",
      "  157:  13247 - \"from\"\n",
      "  159:  13205 - \"this\"\n",
      "  162:  12755 - \"their\"\n",
      "16232:  12554 - \"-\"\n",
      "  154:  12234 - \"them\"\n",
      "16198:  12036 - \"h\"\n",
      "  170:  11516 - \"will\"\n",
      "   91:  10939 - \"we\"\n",
      "  173:  10792 - \"my\"\n",
      "16219:  10286 - \"'\"\n",
      "  156:  10224 - \"at\"\n",
      "  200:   9871 - \"but\"\n",
      "  195:   9642 - \"our\"\n",
      "  183:   9544 - \"him\"\n",
      "  211:   9084 - \"had\"\n",
      "   96:   8907 - \"an\"\n"
     ]
    }
   ],
   "source": [
    "token_counts = count_tokens(input_file, spm_model=model)\n",
    "\n",
    "# Print out our top sampled IDs\n",
    "sorted_counts = sorted(token_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "print(f\"ID     Count    Word\\n{'-'*24}\")\n",
    "for i in range(50):\n",
    "    idx, count = sorted_counts[i]\n",
    "    print(f\"{idx:>5}: {count:>6} - \\\"{model.decode(idx)}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2bd03e7d-0b56-4f07-9e62-8a80b76f867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now - convert our counter dict to a distribution\n",
    "clipped_freq = []\n",
    "for i in range(16834):\n",
    "    clipped_freq.append(token_counts[i] if token_counts[i] > 10 else 0)\n",
    "\n",
    "frequencies_tensor = torch.tensor(clipped_freq, dtype=torch.float)\n",
    "frequencies_tensor = frequencies_tensor / frequencies_tensor.sum()\n",
    "torch.save(frequencies_tensor, '../token_distribution/frequencies_16384.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05452143-bb08-4c9c-a72e-bad60a9c9926",
   "metadata": {},
   "source": [
    "## Inspect Vocabulary\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "adf40c5e-4772-4e56-bcee-ff53c2893674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_print_vocab_samples(vocab_file, start_index=0, num_samples=10):\n",
    "    \"\"\"\n",
    "    Load vocabulary from a SentencePiece .vocab file and print a specified number of samples\n",
    "    starting from a specified index.\n",
    "    \n",
    "    :param vocab_file: Path to the SentencePiece .vocab file\n",
    "    :param start_index: Index to start printing samples from\n",
    "    :param num_samples: Number of vocabulary entries to print\n",
    "    \"\"\"\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab = [line.split('\\t')[0] for line in f.readlines()]  # Extract tokens\n",
    "    \n",
    "    # Ensure start_index and num_samples are within bounds\n",
    "    end_index = min(start_index + num_samples, len(vocab))\n",
    "    \n",
    "    # Print specified samples\n",
    "    for i in range(start_index, end_index):\n",
    "        print(f'Index {i}: {vocab[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a5366258-896f-416c-8986-43dd7ec311ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 14000: ▁Macondo\n",
      "Index 14001: ▁Persian\n",
      "Index 14002: ▁Shortly\n",
      "Index 14003: ▁acquies\n",
      "Index 14004: ▁biggest\n",
      "Index 14005: ▁bullets\n",
      "Index 14006: ▁carpent\n",
      "Index 14007: ▁ceasing\n",
      "Index 14008: ▁conject\n",
      "Index 14009: ▁consign\n",
      "Index 14010: ▁conspic\n",
      "Index 14011: ▁earthly\n",
      "Index 14012: ▁highway\n",
      "Index 14013: ▁induced\n",
      "Index 14014: ▁insight\n",
      "Index 14015: ▁lecture\n",
      "Index 14016: ▁nourish\n",
      "Index 14017: ▁nowhere\n",
      "Index 14018: ▁parched\n",
      "Index 14019: ▁plaster\n",
      "Index 14020: ▁relapse\n",
      "Index 14021: ▁shorter\n",
      "Index 14022: ▁slender\n",
      "Index 14023: ▁sounded\n",
      "Index 14024: ▁spotted\n",
      "Index 14025: ▁stature\n",
      "Index 14026: ▁swiftly\n",
      "Index 14027: ▁uncover\n",
      "Index 14028: etermined\n",
      "Index 14029: ▁Building\n",
      "Index 14030: ▁Chambers\n",
      "Index 14031: ▁Paradise\n",
      "Index 14032: ▁Persians\n",
      "Index 14033: ▁Zubaydah\n",
      "Index 14034: ▁accursed\n",
      "Index 14035: ▁apostles\n",
      "Index 14036: ▁brighter\n",
      "Index 14037: ▁coercion\n",
      "Index 14038: ▁counting\n",
      "Index 14039: ▁depended\n",
      "Index 14040: ▁embodied\n",
      "Index 14041: ▁hallowed\n",
      "Index 14042: ▁heartily\n",
      "Index 14043: ▁hijacker\n",
      "Index 14044: ▁ineffect\n",
      "Index 14045: ▁normally\n",
      "Index 14046: ▁pointing\n",
      "Index 14047: ▁protects\n",
      "Index 14048: ▁schedule\n",
      "Index 14049: ▁sprinkle\n",
      "Index 14050: ▁twilight\n",
      "Index 14051: ▁weakened\n",
      "Index 14052: ▁Christmas\n",
      "Index 14053: ▁Manhattan\n",
      "Index 14054: ▁assertion\n",
      "Index 14055: ▁communion\n",
      "Index 14056: ▁concourse\n",
      "Index 14057: ▁concubine\n",
      "Index 14058: ▁contusion\n",
      "Index 14059: ▁deceitful\n",
      "Index 14060: ▁departing\n",
      "Index 14061: ▁diversity\n",
      "Index 14062: ▁evacuated\n",
      "Index 14063: ▁footsteps\n",
      "Index 14064: ▁fortified\n",
      "Index 14065: ▁forwarded\n",
      "Index 14066: ▁fountains\n",
      "Index 14067: ▁inherited\n",
      "Index 14068: ▁medicines\n",
      "Index 14069: ▁occurring\n",
      "Index 14070: ▁pleasures\n",
      "Index 14071: ▁realistic\n",
      "Index 14072: ▁satellite\n",
      "Index 14073: ▁scheduled\n",
      "Index 14074: ▁slightest\n",
      "Index 14075: ▁socialist\n",
      "Index 14076: ▁wholesome\n",
      "Index 14077: lesiastical\n",
      "Index 14078: ▁Artaxerxes\n",
      "Index 14079: ▁Historical\n",
      "Index 14080: ▁assurances\n",
      "Index 14081: ▁foreigners\n",
      "Index 14082: ▁indulgence\n",
      "Index 14083: ▁invitation\n",
      "Index 14084: ▁neutrality\n",
      "Index 14085: ▁perceiving\n",
      "Index 14086: ▁systematic\n",
      "Index 14087: ▁usefulness\n",
      "Index 14088: ▁chairperson\n",
      "Index 14089: ▁connections\n",
      "Index 14090: ▁probability\n",
      "Index 14091: ▁seventeenth\n",
      "Index 14092: ▁unavoidable\n",
      "Index 14093: ▁architecture\n",
      "Index 14094: ▁circumcision\n",
      "Index 14095: ▁concentrated\n",
      "Index 14096: ▁faithfulness\n",
      "Index 14097: ▁imperfection\n",
      "Index 14098: ▁installations\n",
      "Index 14099: ▁jurisdictions\n",
      "Index 14100: ▁establishments\n",
      "Index 14101: 1-\n",
      "Index 14102: 61\n",
      "Index 14103: –1\n",
      "Index 14104: ▁%\n",
      "Index 14105: ▁«\n",
      "Index 14106: ▁»\n",
      "Index 14107: IRD\n",
      "Index 14108: Ind\n",
      "Index 14109: Our\n",
      "Index 14110: Wor\n",
      "Index 14111: aud\n",
      "Index 14112: nia\n",
      "Index 14113: omf\n",
      "Index 14114: ora\n",
      "Index 14115: oys\n",
      "Index 14116: sky\n",
      "Index 14117: ▁AC\n",
      "Index 14118: ▁Aw\n",
      "Index 14119: ▁Od\n",
      "Index 14120: ▁Si\n",
      "Index 14121: ▁Uz\n",
      "Index 14122: ▁Wy\n",
      "Index 14123: ,000\n",
      "Index 14124: .\"''\n",
      "Index 14125: anah\n",
      "Index 14126: atum\n",
      "Index 14127: cled\n",
      "Index 14128: clop\n",
      "Index 14129: empl\n",
      "Index 14130: heus\n",
      "Index 14131: igue\n",
      "Index 14132: ipes\n",
      "Index 14133: irts\n",
      "Index 14134: ishi\n",
      "Index 14135: itts\n",
      "Index 14136: lord\n",
      "Index 14137: opes\n",
      "Index 14138: orus\n",
      "Index 14139: osts\n",
      "Index 14140: otle\n",
      "Index 14141: phon\n",
      "Index 14142: rait\n",
      "Index 14143: rand\n",
      "Index 14144: rews\n",
      "Index 14145: some\n",
      "Index 14146: uiah\n",
      "Index 14147: ▁4.1\n",
      "Index 14148: ▁Doc\n",
      "Index 14149: ▁Fle\n",
      "Index 14150: ▁Las\n",
      "Index 14151: ▁Sab\n",
      "Index 14152: ▁dug\n",
      "Index 14153: ▁fug\n",
      "Index 14154: ▁mah\n",
      "Index 14155: ▁tho\n",
      "Index 14156: Trans\n",
      "Index 14157: abeth\n",
      "Index 14158: adder\n",
      "Index 14159: aleed\n",
      "Index 14160: books\n",
      "Index 14161: croft\n",
      "Index 14162: hadad\n",
      "Index 14163: inous\n",
      "Index 14164: mates\n",
      "Index 14165: mount\n",
      "Index 14166: nings\n",
      "Index 14167: oding\n",
      "Index 14168: oleon\n",
      "Index 14169: otham\n",
      "Index 14170: quire\n",
      "Index 14171: rance\n",
      "Index 14172: uters\n",
      "Index 14173: ▁''''\n",
      "Index 14174: ▁1972\n",
      "Index 14175: ▁Edin\n",
      "Index 14176: ▁Made\n",
      "Index 14177: ▁Path\n",
      "Index 14178: ▁Resp\n",
      "Index 14179: ▁Spec\n",
      "Index 14180: ▁bapt\n",
      "Index 14181: ▁bowl\n",
      "Index 14182: ▁flaw\n",
      "Index 14183: ▁kine\n",
      "Index 14184: ▁mans\n",
      "Index 14185: ▁quad\n",
      "Index 14186: ▁recl\n",
      "Index 14187: ▁recy\n",
      "Index 14188: ▁rope\n",
      "Index 14189: ▁stem\n",
      "Index 14190: ▁unim\n",
      "Index 14191: ▁wool\n",
      "Index 14192: icious\n",
      "Index 14193: master\n",
      "Index 14194: oneous\n",
      "Index 14195: udence\n",
      "Index 14196: uously\n",
      "Index 14197: ▁Fifth\n",
      "Index 14198: ▁Gomor\n",
      "Index 14199: ▁Maach\n",
      "Index 14200: ▁Nicar\n",
      "Index 14201: ▁Omari\n",
      "Index 14202: ▁Ptole\n",
      "Index 14203: ▁Thine\n",
      "Index 14204: ▁abate\n",
      "Index 14205: ▁acrid\n",
      "Index 14206: ▁agony\n",
      "Index 14207: ▁ambig\n",
      "Index 14208: ▁cords\n",
      "Index 14209: ▁datab\n",
      "Index 14210: ▁haunt\n",
      "Index 14211: ▁hymns\n",
      "Index 14212: ▁indig\n",
      "Index 14213: ▁juice\n",
      "Index 14214: ▁monop\n",
      "Index 14215: ▁posed\n",
      "Index 14216: ▁presc\n",
      "Index 14217: ▁recre\n",
      "Index 14218: ▁remun\n",
      "Index 14219: ▁salut\n",
      "Index 14220: ▁sputa\n",
      "Index 14221: ▁swing\n",
      "Index 14222: ▁tests\n",
      "Index 14223: ▁trips\n",
      "Index 14224: 000,000\n",
      "Index 14225: Richard\n",
      "Index 14226: amental\n",
      "Index 14227: hearted\n",
      "Index 14228: riswell\n",
      "Index 14229: ▁AMERIC\n",
      "Index 14230: ▁Acting\n",
      "Index 14231: ▁Beside\n",
      "Index 14232: ▁Ephron\n",
      "Index 14233: ▁Galile\n",
      "Index 14234: ▁Korean\n",
      "Index 14235: ▁Ramoth\n",
      "Index 14236: ▁Syrian\n",
      "Index 14237: ▁Uzziah\n",
      "Index 14238: ▁Wilson\n",
      "Index 14239: ▁aggreg\n",
      "Index 14240: ▁airlin\n",
      "Index 14241: ▁butter\n",
      "Index 14242: ▁clause\n",
      "Index 14243: ▁counts\n",
      "Index 14244: ▁daring\n",
      "Index 14245: ▁dysent\n",
      "Index 14246: ▁encamp\n",
      "Index 14247: ▁enumer\n",
      "Index 14248: ▁factor\n",
      "Index 14249: ▁fishes\n",
      "Index 14250: ▁goings\n",
      "Index 14251: ▁grains\n",
      "Index 14252: ▁harbor\n",
      "Index 14253: ▁hasted\n",
      "Index 14254: ▁jewels\n",
      "Index 14255: ▁martyr\n"
     ]
    }
   ],
   "source": [
    "vocab_file = '../models/sptokenizer_16384.vocab'\n",
    "load_and_print_vocab_samples(vocab_file, start_index=14000, num_samples=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
