{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ab6d92-0921-4fe1-9c77-61cdd1f3a840",
   "metadata": {},
   "source": [
    "## Using SentencePiece to Train a Tokenizer on a mini-batch of data from enwikisource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747a99da-d86f-44d0-b572-82d57c0eae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import sentencepiece as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cd1373-5c93-45d1-ae61-ce3213f77163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to data and set var for model prefix\n",
    "input_file = '../../data/enwiki_20240320_minibatch.txt'\n",
    "model_prefix = '../models/sptokenizer_256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b7cfc6-12eb-4da8-867f-c25f5ec61cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocabulary have been generated: ../models/sptokenizer_256.model and ../models/sptokenizer_256.vocab\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "sp.SentencePieceTrainer.train(input=input_file,\n",
    "                               model_prefix=model_prefix,\n",
    "                               vocab_size=256,\n",
    "                               character_coverage=0.9995,\n",
    "                               model_type='bpe')\n",
    "\n",
    "print(f'Model and vocabulary have been generated: {model_prefix}.model and {model_prefix}.vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05452143-bb08-4c9c-a72e-bad60a9c9926",
   "metadata": {},
   "source": [
    "## Inspect Vocabulary\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf40c5e-4772-4e56-bcee-ff53c2893674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_print_vocab_samples(vocab_file, start_index=0, num_samples=10):\n",
    "    \"\"\"\n",
    "    Load vocabulary from a SentencePiece .vocab file and print a specified number of samples\n",
    "    starting from a specified index.\n",
    "    \n",
    "    :param vocab_file: Path to the SentencePiece .vocab file\n",
    "    :param start_index: Index to start printing samples from\n",
    "    :param num_samples: Number of vocabulary entries to print\n",
    "    \"\"\"\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab = [line.split('\\t')[0] for line in f.readlines()]  # Extract tokens\n",
    "    \n",
    "    # Ensure start_index and num_samples are within bounds\n",
    "    end_index = min(start_index + num_samples, len(vocab))\n",
    "    \n",
    "    # Print specified samples\n",
    "    for i in range(start_index, end_index):\n",
    "        print(f'Index {i}: {vocab[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5366258-896f-416c-8986-43dd7ec311ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: <unk>\n",
      "Index 1: <s>\n",
      "Index 2: </s>\n",
      "Index 3: ▁t\n",
      "Index 4: he\n",
      "Index 5: ▁a\n",
      "Index 6: ▁the\n",
      "Index 7: in\n",
      "Index 8: ▁o\n",
      "Index 9: re\n",
      "Index 10: nd\n",
      "Index 11: ▁s\n",
      "Index 12: ▁w\n",
      "Index 13: er\n",
      "Index 14: at\n",
      "Index 15: on\n",
      "Index 16: ▁of\n",
      "Index 17: it\n",
      "Index 18: ▁b\n",
      "Index 19: is\n",
      "Index 20: en\n",
      "Index 21: ▁and\n",
      "Index 22: ▁c\n",
      "Index 23: ▁f\n",
      "Index 24: ▁m\n",
      "Index 25: es\n",
      "Index 26: or\n",
      "Index 27: ed\n",
      "Index 28: ▁p\n",
      "Index 29: ▁h\n",
      "Index 30: ou\n",
      "Index 31: ▁th\n",
      "Index 32: ▁to\n",
      "Index 33: ▁in\n",
      "Index 34: al\n",
      "Index 35: an\n",
      "Index 36: ing\n",
      "Index 37: ar\n",
      "Index 38: ▁d\n",
      "Index 39: ion\n",
      "Index 40: ic\n",
      "Index 41: ▁n\n",
      "Index 42: as\n",
      "Index 43: le\n",
      "Index 44: ▁be\n",
      "Index 45: om\n",
      "Index 46: ll\n",
      "Index 47: ent\n",
      "Index 48: ▁I\n",
      "Index 49: ▁e\n",
      "Index 50: ▁l\n",
      "Index 51: ▁re\n",
      "Index 52: ve\n",
      "Index 53: ot\n",
      "Index 54: ▁u\n",
      "Index 55: st\n",
      "Index 56: se\n",
      "Index 57: ▁A\n",
      "Index 58: ▁that\n",
      "Index 59: ut\n",
      "Index 60: ▁g\n",
      "Index 61: ce\n",
      "Index 62: ▁T\n",
      "Index 63: ▁he\n",
      "Index 64: ct\n",
      "Index 65: ▁for\n",
      "Index 66: id\n",
      "Index 67: ▁wh\n",
      "Index 68: ly\n",
      "Index 69: im\n",
      "Index 70: ro\n",
      "Index 71: ur\n",
      "Index 72: ▁ha\n",
      "Index 73: ld\n",
      "Index 74: ▁is\n",
      "Index 75: ir\n",
      "Index 76: ation\n",
      "Index 77: ig\n",
      "Index 78: ith\n",
      "Index 79: ▁S\n",
      "Index 80: ay\n",
      "Index 81: ▁sh\n",
      "Index 82: ver\n",
      "Index 83: ▁it\n",
      "Index 84: ow\n",
      "Index 85: et\n",
      "Index 86: all\n",
      "Index 87: ▁with\n",
      "Index 88: ▁on\n",
      "Index 89: ▁as\n",
      "Index 90: am\n",
      "Index 91: ▁we\n",
      "Index 92: her\n",
      "Index 93: ▁not\n",
      "Index 94: ▁y\n",
      "Index 95: ra\n",
      "Index 96: ▁an\n",
      "Index 97: ch\n",
      "Index 98: ▁or\n",
      "Index 99: ter\n",
      "Index 100: il\n",
      "Index 101: ▁The\n",
      "Index 102: ad\n",
      "Index 103: ▁C\n",
      "Index 104: ▁con\n",
      "Index 105: ill\n",
      "Index 106: nt\n",
      "Index 107: ht\n",
      "Index 108: if\n",
      "Index 109: ▁st\n",
      "Index 110: ▁his\n",
      "Index 111: ▁by\n",
      "Index 112: od\n",
      "Index 113: ore\n",
      "Index 114: ess\n",
      "Index 115: ain\n",
      "Index 116: ▁de\n",
      "Index 117: th\n",
      "Index 118: ich\n",
      "Index 119: ▁shall\n",
      "Index 120: ▁su\n",
      "Index 121: ▁was\n",
      "Index 122: ▁B\n",
      "Index 123: ▁se\n",
      "Index 124: ▁L\n",
      "Index 125: ▁com\n",
      "Index 126: ment\n",
      "Index 127: ▁fr\n",
      "Index 128: ▁which\n",
      "Index 129: ▁me\n",
      "Index 130: ▁all\n",
      "Index 131: res\n",
      "Index 132: ak\n",
      "Index 133: ▁have\n",
      "Index 134: ers\n",
      "Index 135: ▁pro\n",
      "Index 136: ▁M\n",
      "Index 137: ▁P\n",
      "Index 138: est\n",
      "Index 139: ri\n",
      "Index 140: ate\n",
      "Index 141: pe\n",
      "Index 142: ▁they\n",
      "Index 143: and\n",
      "Index 144: ol\n",
      "Index 145: ul\n",
      "Index 146: our\n",
      "Index 147: ▁are\n",
      "Index 148: ight\n",
      "Index 149: ity\n",
      "Index 150: ab\n",
      "Index 151: ▁H\n",
      "Index 152: ▁k\n",
      "Index 153: ▁And\n",
      "Index 154: ▁them\n",
      "Index 155: ▁G\n",
      "Index 156: ▁at\n",
      "Index 157: ▁from\n",
      "Index 158: ould\n",
      "Index 159: ▁this\n",
      "Index 160: ▁1\n",
      "Index 161: ▁W\n",
      "Index 162: ''\n",
      "Index 163: ▁their\n",
      "Index 164: ies\n",
      "Index 165: red\n",
      "Index 166: ▁ex\n",
      "Index 167: ist\n",
      "Index 168: ▁F\n",
      "Index 169: ge\n",
      "Index 170: ive\n",
      "Index 171: ▁will\n",
      "Index 172: ▁ne\n",
      "Index 173: ac\n",
      "Index 174: ▁my\n",
      "Index 175: qu\n",
      "Index 176: us\n",
      "Index 177: ▁\n",
      "Index 178: e\n",
      "Index 179: t\n",
      "Index 180: a\n",
      "Index 181: o\n",
      "Index 182: n\n",
      "Index 183: i\n",
      "Index 184: s\n",
      "Index 185: h\n",
      "Index 186: r\n",
      "Index 187: d\n",
      "Index 188: l\n",
      "Index 189: c\n",
      "Index 190: u\n",
      "Index 191: f\n",
      "Index 192: m\n",
      "Index 193: p\n",
      "Index 194: w\n",
      "Index 195: y\n",
      "Index 196: g\n",
      "Index 197: ,\n",
      "Index 198: b\n",
      "Index 199: v\n",
      "Index 200: .\n",
      "Index 201: k\n",
      "Index 202: I\n",
      "Index 203: A\n",
      "Index 204: T\n",
      "Index 205: S\n",
      "Index 206: '\n",
      "Index 207: ;\n",
      "Index 208: C\n",
      "Index 209: 1\n",
      "Index 210: x\n",
      "Index 211: :\n",
      "Index 212: O\n",
      "Index 213: M\n",
      "Index 214: B\n",
      "Index 215: L\n",
      "Index 216: R\n",
      "Index 217: P\n",
      "Index 218: D\n",
      "Index 219: -\n",
      "Index 220: 2\n",
      "Index 221: H\n",
      "Index 222: j\n",
      "Index 223: G\n",
      "Index 224: W\n",
      "Index 225: 0\n",
      "Index 226: N\n",
      "Index 227: E\n",
      "Index 228: F\n",
      "Index 229: q\n",
      "Index 230: )\n",
      "Index 231: (\n",
      "Index 232: <\n",
      "Index 233: >\n",
      "Index 234: J\n",
      "Index 235: z\n",
      "Index 236: 3\n",
      "Index 237: \"\n",
      "Index 238: 9\n",
      "Index 239: 4\n",
      "Index 240: 8\n",
      "Index 241: U\n",
      "Index 242: /\n",
      "Index 243: 5\n",
      "Index 244: ?\n",
      "Index 245: 7\n",
      "Index 246: &\n",
      "Index 247: 6\n",
      "Index 248: Y\n",
      "Index 249: ’\n",
      "Index 250: K\n",
      "Index 251: #\n",
      "Index 252: —\n",
      "Index 253: V\n",
      "Index 254: “\n",
      "Index 255: ”\n"
     ]
    }
   ],
   "source": [
    "vocab_file = '../models/sptokenizer_256.vocab'\n",
    "load_and_print_vocab_samples(vocab_file, start_index=0, num_samples=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
