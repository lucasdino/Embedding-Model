{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772186fd-ff39-4781-b1c0-23afcc15b5b9",
   "metadata": {},
   "source": [
    "# Import Dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e0e27b-d2a7-4a8a-9147-747b717792a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import our various classes\n",
    "from gpt import GPTLanguageModel\n",
    "from dataloader.dataloader import MyDataLoader\n",
    "from tokenizer.tokenizer import MyTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da397f-29a6-4585-a1bf-fc84f2a071d2",
   "metadata": {},
   "source": [
    "# Defining our ModelTrainer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e49e5f9-3a6e-4b03-9694-1c4722590e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our ModelTrainer\n",
    "class ModelTrainer():\n",
    "    def __init__(self, model, tokenizer, train_dl, test_dl, train_params, device):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dl = train_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.train_params = train_params\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def train_model(self, num_saves=1):\n",
    "        \"\"\" \n",
    "            Function to train our model based on params defined in 'self.train_params'\n",
    "\n",
    "        Inputs:\n",
    "            num_saves: (int) define number of times the model weights will be saved during training\n",
    "        \n",
    "        \"\"\"\n",
    "        print(sum(p.numel() for p in self.model.parameters())/1e6, 'M parameters')\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), self.train_params['learning_rate'])\n",
    "    \n",
    "        max_iters = self.train_params['max_iters']\n",
    "        eval_interval = self.train_params['eval_interval']\n",
    "        save_interval = int(max_iters / num_saves)\n",
    "        loss_tensor = torch.zeros((int(max_iters / eval_interval) + 1, 3))\n",
    "        \n",
    "        iters = 0\n",
    "        for _ in range(self.train_params['max_epochs']):\n",
    "            for data in self.train_dl:\n",
    "                tokens = torch.tensor(self.tokenizer.encode_as_ids(data[0][0]), dtype=torch.long)\n",
    "                for (X, Y) in self._yield_batch(tokens, util_rate=0.5):\n",
    "        \n",
    "                    if (iters % eval_interval == 0) or (iters == max_iters - 1):\n",
    "                        losses = self._estimate_loss()\n",
    "                        print(f\"{iters:>8}/{max_iters:>8}: Train - {losses[0]:>7.4f}, Test - {losses[1]:>7.4f}\")\n",
    "                        if (iters != max_iters-1):\n",
    "                            loss_tensor[int(iters / eval_interval), :] = torch.tensor([iters, losses[0], losses[1]])\n",
    "                        else:\n",
    "                            loss_tensor[-1, :] = torch.tensor([iters, losses[0], losses[1]])\n",
    "                            \n",
    "                    logits, loss = self.model(X, Y)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    iters += 1\n",
    "        \n",
    "                    if iters % save_interval == 0:\n",
    "                        base_dir = os.path.join(os.path.dirname(os.getcwd()), 'transformer')\n",
    "                        filename = datetime.now().strftime(\"GPTLanguageModel_%d_%m_%Y_%M_%H\")\n",
    "                        self.model.save_model(directory=os.path.join(base_dir, 'model_weights'), filename=filename + '.model')\n",
    "                        csv_path = os.path.join(base_dir, 'loss_results', filename + '.csv')\n",
    "                        np.savetxt(csv_path, loss_tensor.numpy(), delimiter=\",\", header=\"Iteration,Train Loss,Test Loss\", comments='')\n",
    "                        print(f\"Loss data saved to {csv_path} as CSV\")\n",
    "        \n",
    "                    if iters == max_iters:\n",
    "                        print(f\"\\nTraining Complete - Sample Generation:\\n{'-'*60}\")\n",
    "                        starting_tokens = self.tokenizer.encode_as_ids(\"Once upon a time, there was a frog\")\n",
    "                        print(self.tokenizer.decode(self.model.generate(starting_tokens, max_new_tokens=1000)))\n",
    "                        return\n",
    "        \n",
    "    \n",
    "    def _yield_batch(self, tokens, util_rate=0.5, iters_per_batch=None):\n",
    "        \"\"\"\n",
    "            Generator function that takes in text and returns a number of batches for each dataset based on the utilization rate or specific_iter.\n",
    "    \n",
    "            Inputs:\n",
    "                tokens:           (List(int)) The text provided by the dataloader, encoded as IDs from the tokenizer\n",
    "                util_rate:        (float) Value from (0, 1] that specifies the % of possible batches that are generated before moving to next sample\n",
    "                iters_per_batch:  (int) If set to none, we use util rate to determine num batches from this data. If set to an int, we use that number of batches.\n",
    "    \n",
    "            Yields (generator function) batches of data in the form of GPU-mounted pytorch tensors until util_rate is tripped.\n",
    "        \"\"\"\n",
    "        # Store vars for convenient use and get shuffled indices for starting tokens\n",
    "        context_size = self.model.params['context_size']\n",
    "        batch_size = self.model.params['batch_size']\n",
    "        sample_length = len(tokens) - context_size\n",
    "        shuffled_indices = torch.randperm(sample_length)\n",
    "        \n",
    "        # Compute number of batches we'll yield\n",
    "        if iters_per_batch == None:\n",
    "            num_batches = int( (sample_length * util_rate) // batch_size ) \n",
    "        else:\n",
    "            num_batches = min(iters_per_batch, int(sample_length // batch_size)) \n",
    "    \n",
    "        # Generate batches\n",
    "        for batch in range(num_batches):\n",
    "            ix = shuffled_indices[(batch * batch_size):((batch+1) * batch_size)].tolist()    # List (length = B) of starting indices\n",
    "            X = torch.stack([tokens[i:(i+context_size)] for i in ix])                        # [B,T] batch of inputs\n",
    "            Y = torch.stack([tokens[(i+1):(i+context_size+1)] for i in ix])                  # [B,T] batch of outputs\n",
    "            X, Y = X.to(self.device), Y.to(self.device)\n",
    "            yield X, Y\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _estimate_loss(self):\n",
    "        \"\"\" Function to estimate our loss (train and test). Returns the mean train and test loss (as a tuple) \"\"\"\n",
    "        train_loss = torch.tensor(0.0, device=self.device)\n",
    "        test_loss = torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "        self.model.eval()\n",
    "        for sample_num, data in enumerate(self.train_dl):\n",
    "            train_tokens = torch.tensor(self.tokenizer.encode_as_ids(data[0][0]), dtype=torch.long, device=self.device)\n",
    "            for (X, Y) in self._yield_batch(tokens=train_tokens, iters_per_batch=self.train_params['iters_per_eval_sample']):\n",
    "                _, loss = self.model(X, Y)\n",
    "                train_loss += loss\n",
    "            if sample_num == (self.train_params['eval_samples'] - 1):\n",
    "                break  # We've gone through 'n_samples' training samples, each for 'iters_per_sample' iterations of batch_size 'params['batch_size']'\n",
    "    \n",
    "        for sample_num, data in enumerate(self.test_dl):\n",
    "            test_tokens = torch.tensor(self.tokenizer.encode_as_ids(data[0][0]), dtype=torch.long, device=self.device)\n",
    "            for (X, Y) in self._yield_batch(tokens=test_tokens, iters_per_batch=self.train_params['iters_per_eval_sample']):\n",
    "                _, loss = self.model(X, Y)\n",
    "                test_loss += loss\n",
    "            if sample_num == (self.train_params['eval_samples'] - 1):\n",
    "                break \n",
    "        self.model.train()\n",
    "    \n",
    "        # Move the final loss calculation to CPU just before returning the result\n",
    "        num_samples = self.train_params['eval_samples'] * self.train_params['iters_per_eval_sample'] * (self.model.params['batch_size'] ** 0.5)\n",
    "        return (train_loss / num_samples).to('cpu').item(), (test_loss / num_samples).to('cpu').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d8d0e-0a6e-444d-8fa5-9844b412813c",
   "metadata": {},
   "source": [
    "# Train Our Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfa4153e-0ebd-49c8-a988-419416163723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'batch_size': 32,\n",
    "    'context_size': 32,\n",
    "    'embed_dim': 512,\n",
    "    'vocab_size': 16384,\n",
    "    'n_head': 6,\n",
    "    'n_layer': 6,\n",
    "    'dropout': 0.2\n",
    "}\n",
    "train_params = {\n",
    "    'max_iters': 1e4,\n",
    "    'eval_interval': 1e3,\n",
    "    'learning_rate': 3e-4,\n",
    "    'max_epochs': 1,\n",
    "    'eval_samples': 20,\n",
    "    'iters_per_eval_sample': 2\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cbfa5d0-cca3-473e-bdba-e03965199873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully from C:\\Users\\lucas\\Desktop\\Lucas\\Coding\\ML Projects\\Embedding Model\\embedding_model\\embeddings\\v16384_d512_4_26_24.pth.\n"
     ]
    }
   ],
   "source": [
    "# Import our dataloader\n",
    "dl = MyDataLoader(\n",
    "    promptuser=False, \n",
    "    batch_size=1, \n",
    "    shuffle=True)    \n",
    "\n",
    "train_dataloader = dl.get_train_dataloader()\n",
    "test_dataloader = dl.get_test_dataloader()\n",
    "\n",
    "# Import tokenizer\n",
    "tokenizer = MyTokenizer()\n",
    "\n",
    "# Instantiate GPT Model\n",
    "model = GPTLanguageModel(model_params, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd99cae-4ab8-4081-b2cc-f35624e23cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.69152 M parameters\n",
      "        0/10000.0: Train -     0.3058, Test -     0.3052\n",
      "     1000/10000.0: Train -     0.2397, Test -     0.2360\n",
      "     2000/10000.0: Train -     0.2223, Test -     0.2223\n"
     ]
    }
   ],
   "source": [
    "# Train our model\n",
    "trainer = ModelTrainer(model = model, \n",
    "                       tokenizer = tokenizer, \n",
    "                       train_dl = train_dataloader, \n",
    "                       test_dl = test_dataloader, \n",
    "                       train_params = train_params, \n",
    "                       device = device)\n",
    "\n",
    "trainer.train_model(num_saves = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f98df-f412-49e1-8bcf-380ec5f92d1a",
   "metadata": {},
   "source": [
    "# Helpful for Debugging / Confirmation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72c56a-10ee-47f1-ac47-ccbfd6efbd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to ensure that our embeddings are working as expected\n",
    "embeds = model.token_embedding_table.weight\n",
    "str = \"boy\"\n",
    "tok = tokenizer.encode_as_ids(str)[0]\n",
    "emb_str = embeds[tok:tok+1, :].T\n",
    "_, indices = torch.topk((embeds@emb_str).flatten(), 11)\n",
    "for i, idx in enumerate(indices):\n",
    "    print(f\"Tok {i:>2}: \\\"{tokenizer.decode(idx.tolist())}\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
