# Embedding Model
---
In the process of learning about embeddings, I'm building my own embedding model based on Word2Vec. Included is:
1. Tokenizer: Using Andrej Karpathy's tutorial as a starting point, I built an optimized bytepair encoding tokenizer (BPETokenizer) that will be used to tokenize our data
2. Embedding Model: Training an embedding model based on Word2Vec and other common methods. Using defined tokenizer for my implementations.
3. GPT: Training a transformer to see how effective I can model tinyshakespeare and wikipedia text.