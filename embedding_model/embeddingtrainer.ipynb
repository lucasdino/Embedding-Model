{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0948d168-bc6d-49f0-a0a8-46ffef674a9c",
   "metadata": {},
   "source": [
    "# Embedding Trainer\n",
    "---\n",
    "Main script to train our various embedding models. What we'll do is import our 'Embedding Model' objects that we'll define in other files - and this code will be the training loop and will save the results to various folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ff5d43-dfc7-4007-9d54-8527bbf69b22",
   "metadata": {},
   "source": [
    "## Create our Dataloader and Tokenizer\n",
    "---\n",
    "We already defined these and trained the tokenizer in the other files. So, let's go ahead and instantiate these as a first step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84fc05-e0dc-4820-b6d5-7593b3569bda",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce4ce0c-327e-4491-a5d2-97dae3584ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow for easy access of other packages in this directory, let's first nav to the project root\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49e9ebe-1710-4b4b-9828-c79e96dd34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.dataloader import MyDataLoader\n",
    "from tokenizer.tokenizer import MyTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18bf2d-67c0-4610-bee2-c956cb56b126",
   "metadata": {},
   "source": [
    "### Instantiate our Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b78d90c-3957-4114-90e7-8bf6a60c19c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars: 44986\n",
      "============================================================\n",
      "Mr. President and fellow citizens of New York: -\n",
      "\n",
      "The facts with which I shall deal this evening are mainly old and familiar; nor is there anything new in the general use I shall make of them. If there shall be any novelty, it will be in the mode of presenting the facts, and the inferences and observations following that presentation.\n",
      "\n",
      "In his speech last autumn, at Columbus, Ohio, as reported in \"The New-York Times,\" Senator Douglas said:\n",
      "\n",
      "\"Our fathers, when they framed the Government under whic\n"
     ]
    }
   ],
   "source": [
    "dl = MyDataLoader(promptuser=False, batch_size=1, shuffle=True)    # By setting promptuser=False, we just use the 'enwiki_articles_20240320_mini' dataset (50MB)\n",
    "train_dataloader = dl.get_train_dataloader()\n",
    "test_dataloader = dl.get_test_dataloader()\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    sample_data = batch[0][0]\n",
    "    break\n",
    "\n",
    "print(f\"Number of chars: {len(sample_data)}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(sample_data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbbbd04f-e661-495b-b1e3-0bd456cea9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #1\n",
      "Number of chars: 10230\n",
      "==================================================\n",
      "<h2> Case document</h2>\n",
      "1.\tThese proceedings are now cited as Re Kevin : Validity of Marriage of Transsexual (2001) FamCA 1074 and (2001) FLC 93-087 (\"Re Kevin\"). Justice Chisholm's original decision, granting a Declaration of Validity of Marriage in\n",
      "==================================================\n",
      "\n",
      "Sample #2\n",
      "Number of chars: 8842\n",
      "==================================================\n",
      ", , , ,  distinguished guests and my fellow citizens:\n",
      "\n",
      "The peaceful transfer of authority is rare in history, yet common in our country. With a simple oath, we affirm old traditions and make new beginnings.\n",
      "\n",
      "As I begin, I thank President Clinton for \n",
      "==================================================\n",
      "\n",
      "Sample #3\n",
      "Number of chars: 15567\n",
      "==================================================\n",
      "They were not long in reaching the barracks, for the officer who \n",
      "commanded the party was desirous to avoid rousing the people by the \n",
      "display of military force in the streets, and was humanely anxious \n",
      "to give as little opportunity as possible for a\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Test Dataset Samples:\n",
      "==================================================\n",
      "Number of chars: 1548\n",
      "==================================================\n",
      "Shrines. Come along. It's rather picturesque. A variant on Velasquez's ''Les Lanzas.'''\n",
      "\n",
      "Reluctantly Jill followed him round to the high ''grille'' which yielded to his thrust.\n",
      "\n",
      "They found themselves in a gravelled, orderly necropolis, a ''Galerie La\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dl.print_samples(num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d56a8-872e-45bc-93e6-cda4428f947a",
   "metadata": {},
   "source": [
    "### Instantiate our Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f352301-58ec-4c6a-ba92-101ea383444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136, 186, 200, 137, 131, 66, 47, 21, 23, 178, 46, 84, 22, 17, 183, 235, 20, 184, 16, 177, 226, 178, 194, 177, 248, 26, 201, 211, 177, 219, 101, 23, 180, 64, 184, 87, 128, 48, 119, 116, 34, 159, 49, 199, 20, 36, 147, 24, 115, 68, 8, 73, 21, 23, 90, 100, 183, 37, 207, 41, 26, 74, 6, 9, 96, 195, 117, 36, 172, 194, 33, 6, 60, 20, 13, 34, 54, 56, 48, 119, 24, 132, 178, 16, 154, 200, 48, 191, 6, 186]\n",
      "------------------------------------------------------------\n",
      "['▁M', 'r', '.', '▁P', 'res', 'id', 'ent', '▁and', '▁f', 'e', 'll', 'ow', '▁c', 'it', 'i', 'z', 'en', 's', '▁of', '▁', 'N', 'e', 'w', '▁', 'Y', 'or', 'k', ':', '▁', '-', '▁The', '▁f', 'a', 'ct', 's', '▁with', '▁which', '▁I', '▁shall', '▁de', 'al', '▁this', '▁e', 'v', 'en', 'ing', '▁are', '▁m', 'ain', 'ly', '▁o', 'ld', '▁and', '▁f', 'am', 'il', 'i', 'ar', ';', '▁n', 'or', '▁is', '▁the', 're', '▁an', 'y', 'th', 'ing', '▁ne', 'w', '▁in', '▁the', '▁g', 'en', 'er', 'al', '▁u', 'se', '▁I', '▁shall', '▁m', 'ak', 'e', '▁of', '▁them', '.', '▁I', 'f', '▁the', 'r']\n",
      "------------------------------------------------------------\n",
      "Mr. President and fellow citizens of New York: - The facts with which I shall deal this evening are mainly old and familiar; nor is there anything new in the general use I shall make of them. If ther\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MyTokenizer()\n",
    "tk_vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "# Ensure our tokenizer is running properly\n",
    "chars_to_print = 200\n",
    "print(tokenizer.encode_as_ids(sample_data[:chars_to_print]))\n",
    "print(f\"{'-'*60}\")\n",
    "print(tokenizer.encode_as_pieces(sample_data[:chars_to_print]))\n",
    "print(f\"{'-'*60}\")\n",
    "print(tokenizer.decode(tokenizer.encode_as_ids(sample_data[:chars_to_print])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0484a7-8dbf-4e9f-a809-4f3936f30949",
   "metadata": {},
   "source": [
    "## Define our Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ade923d-3208-4b29-a5ed-e8f1dea53195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d4be45-a413-44c7-bb51-15f0d0431bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d5caaa-7fa2-4591-a07e-7d469f9c799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _yield_CBOW_batch(text, batch_size, n_window, util_rate, vocab_size=tk_vocab_size, device=device, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "        Generator function that takes in text and returns a number of batches for each dataset based on the utilization rate specified. To be used in a CBOW model.\n",
    "\n",
    "        Inputs:\n",
    "            text:       (string) The text provided by the dataloader\n",
    "            batch_size: (int) Number of samples to return in each batch\n",
    "            n_window:   (int) Size of our context window for the CBOW model. I.e., if n_window=4, then we will use the left 4 words and right 4 words to predict our target word\n",
    "            util_rate:  (float) Value from (0, 1] that specifies the % of possible batches that are generated before moving to next sample\n",
    "            vocab_size: (int) size of our tokenizer vocabulary\n",
    "            device:     Pytorch device (e.g., cuda / cpu)\n",
    "            tokenizer:  Our defined tokenizer (above). encode_as_ids(text) returns a 1-D python list of tokens\n",
    "\n",
    "        Yields (generator function) batches of data in the form of GPU-mounted pytorch tensors until util_rate is tripped.\n",
    "    \"\"\"\n",
    "    tokens = torch.tensor(tokenizer.encode_as_ids(text), device=device)\n",
    "    len_tokens = len(tokens)\n",
    "    num_possible_pairs = len_tokens - (2 * n_window)\n",
    "    num_batches = int((num_possible_pairs * util_rate) // batch_size)\n",
    "    \n",
    "    center_indices = torch.arange(n_window, len_tokens - n_window, device=device)\n",
    "    center_indices = center_indices[torch.randperm(center_indices.size(0))][:num_batches*batch_size]\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch_center_indices = center_indices[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        # Initialize the context and target tensors\n",
    "        context_tensor = torch.zeros(batch_size, vocab_size, device=device)\n",
    "        target_tensor = torch.zeros(batch_size, vocab_size, device=device)\n",
    "        \n",
    "        for idx, center_idx in enumerate(batch_center_indices):\n",
    "            # For each center word index, create a context window\n",
    "            context_indices = torch.cat((tokens[center_idx - n_window:center_idx], tokens[center_idx + 1:center_idx + 1 + n_window]))\n",
    "            \n",
    "            # Update the context_tensor for all context words (summing up one-hot vectors)\n",
    "            for context_idx in context_indices:\n",
    "                context_tensor[idx, context_idx] += 1\n",
    "            normalized_context_tensor = context_tensor / (n_window*2)   # we want the sum of each context tensor to be 1 (so becomes average of the associated word vecs)\n",
    "            \n",
    "            # Update the target_tensor\n",
    "            target_tensor[idx, tokens[center_idx]] = 1\n",
    "        \n",
    "        yield normalized_context_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c32c55-266e-4417-ac11-3e04f8f99853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "------------------------------------------------------------\n",
      "tensor(8.)\n",
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "batch_size=8\n",
    "n_window=4\n",
    "util_rate=0.4\n",
    "\n",
    "for batch, (context, target) in enumerate(_yield_CBOW_batch(sample_data, batch_size, n_window, util_rate, device=device, tokenizer=tokenizer)):\n",
    "    print(f\"Batch {batch+1}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(context.sum())\n",
    "    print(target.sum())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8baec6a3-9c0b-4b6f-852d-e6c91d9b1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next - let's keep track of our loss data\n",
    "def _estimate_loss(model, n_iters, device=device):\n",
    "    \"\"\"\n",
    "        Function to estimate our loss (train and test) that we can call\n",
    "\n",
    "        Inputs:\n",
    "            model:   Pytorch sequential model\n",
    "            n_iters: (int) Specify number of iterations to compute loss over\n",
    "            device:  Pytorch device (cuda / cpu)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b79f7-2cce-4228-957d-5e02fd728ced",
   "metadata": {},
   "source": [
    "## Import and Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77b5d9a3-5e8f-4c21-a368-1da0a1c651a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pytorch Dependencies\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "497fba5e-bae1-4cf3-acdc-ce5968c3143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of a simple LBL model as laid out in https://www.cs.toronto.edu/~amnih/papers/hlbl_final.pdf (Introduces Hierarchal Softmax)\n",
    "class LogBilinearModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, device):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(vocab_size, embed_dim),\n",
    "            nn.Linear(embed_dim, vocab_size)\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "            Single forward pass, takes in matrix 'x' [B x vocab_size] where 'x' is a multi-hot encoding relating to the tokens in the context window.\n",
    "            Targets (optional for loss calculation) is a [B x V] vector\n",
    "        \"\"\"\n",
    "        logits = self.net(x)  # [B x Vocab_size(V)] @ [V x V] -> [B x V]\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, V = logits.shape\n",
    "            logits_flat = logits.view(B*V)\n",
    "            targets = targets.view(B*V)\n",
    "            loss = F.cross_entropy(logits_flat, targets)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5557ee7-421f-48fa-bca3-cc404503938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "batch_size=8\n",
    "n_window=4\n",
    "util_rate=0.5\n",
    "learning_rate = 3e-4\n",
    "embed_dim = 128\n",
    "\n",
    "# Define a simple training loop\n",
    "def train_model(modelclass, train_dl, test_dl, tokenizer, device, saveweights=True):\n",
    "    \"\"\"\n",
    "        Main training loop for our model that we specify\n",
    "    \"\"\"\n",
    "    model = modelclass(tk_vocab_size, embed_dim, device)\n",
    "    m = model.to(device)\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_hist = []\n",
    "    temp_loss_hist = []\n",
    "    iter = 0\n",
    "\n",
    "    for sample in train_dl:\n",
    "        for (context, targets) in _yield_CBOW_batch(sample[0][0], batch_size, n_window, util_rate, tk_vocab_size, device=device, tokenizer=tokenizer):\n",
    "            iter += 1\n",
    "            logits, loss = model(context, targets)\n",
    "            temp_loss_hist.append(loss)\n",
    "            \n",
    "            if iter % 1000 == 0:\n",
    "                ave_loss = torch.tensor(temp_loss_hist).mean()\n",
    "                loss_hist.append(ave_loss)\n",
    "                temp_loss_hist = []\n",
    "                if iter % 10000 == 0:\n",
    "                    print(f\"Iter: {iter} - Loss: {ave_loss:.3f}\")\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if iter>1e7:\n",
    "            return loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b111a58-54b5-4b26-9b94-2c85302c8440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06592 M parameters\n",
      "Iter: 10000 - Loss: 56.110\n",
      "Iter: 20000 - Loss: 55.149\n",
      "Iter: 30000 - Loss: 54.297\n",
      "Iter: 40000 - Loss: 51.408\n",
      "Iter: 50000 - Loss: 52.942\n",
      "Iter: 60000 - Loss: 51.462\n",
      "Iter: 70000 - Loss: 49.600\n",
      "Iter: 80000 - Loss: 50.091\n",
      "Iter: 90000 - Loss: 50.147\n",
      "Iter: 100000 - Loss: 51.474\n",
      "Iter: 110000 - Loss: 51.336\n",
      "Iter: 120000 - Loss: 49.860\n",
      "Iter: 130000 - Loss: 50.156\n",
      "Iter: 140000 - Loss: 50.054\n",
      "Iter: 150000 - Loss: 49.193\n",
      "Iter: 160000 - Loss: 47.932\n",
      "Iter: 170000 - Loss: 49.589\n",
      "Iter: 180000 - Loss: 51.657\n",
      "Iter: 190000 - Loss: 46.651\n",
      "Iter: 200000 - Loss: 49.788\n",
      "Iter: 210000 - Loss: 49.816\n",
      "Iter: 220000 - Loss: 49.551\n",
      "Iter: 230000 - Loss: 49.789\n",
      "Iter: 240000 - Loss: 47.909\n",
      "Iter: 250000 - Loss: 48.993\n",
      "Iter: 260000 - Loss: 49.623\n",
      "Iter: 270000 - Loss: 49.286\n",
      "Iter: 280000 - Loss: 48.986\n",
      "Iter: 290000 - Loss: 50.074\n",
      "Iter: 300000 - Loss: 50.057\n",
      "Iter: 310000 - Loss: 42.042\n",
      "Iter: 320000 - Loss: 48.016\n",
      "Iter: 330000 - Loss: 48.414\n",
      "Iter: 340000 - Loss: 50.513\n",
      "Iter: 350000 - Loss: 49.444\n",
      "Iter: 360000 - Loss: 45.363\n",
      "Iter: 370000 - Loss: 49.983\n",
      "Iter: 380000 - Loss: 49.732\n",
      "Iter: 390000 - Loss: 49.168\n",
      "Iter: 400000 - Loss: 49.141\n",
      "Iter: 410000 - Loss: 48.963\n",
      "Iter: 420000 - Loss: 48.838\n",
      "Iter: 430000 - Loss: 47.578\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Running our training loop\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLogBilinearModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(modelclass, train_dl, test_dl, tokenizer, device, saveweights)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (context, targets) \u001b[38;5;129;01min\u001b[39;00m _yield_CBOW_batch(sample[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], batch_size, n_window, util_rate, tk_vocab_size, device\u001b[38;5;241m=\u001b[39mdevice, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer):\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     25\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(context, targets)\n",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m, in \u001b[0;36m_yield_CBOW_batch\u001b[1;34m(text, batch_size, n_window, util_rate, vocab_size, device, tokenizer)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Update the context_tensor for all context words (summing up one-hot vectors)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m context_idx \u001b[38;5;129;01min\u001b[39;00m context_indices:\n\u001b[1;32m---> 37\u001b[0m     context_tensor[idx, context_idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     38\u001b[0m normalized_context_tensor \u001b[38;5;241m=\u001b[39m context_tensor \u001b[38;5;241m/\u001b[39m (n_window\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)   \u001b[38;5;66;03m# we want the sum of each context tensor to be 1 (so becomes average of the associated word vecs)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Update the target_tensor\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running our training loop\n",
    "loss = train_model(LogBilinearModel, train_dataloader, test_dataloader, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
