{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bb7d46-f65f-47c0-84f1-0ea75770c48d",
   "metadata": {},
   "source": [
    "## Using the GPT-2 Tokenizer from Hugging Face\n",
    "---\n",
    "Goal is to be able to tokenize large amounts of text so that I can train a word embedding model on the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40d00f-ab0a-4fda-bc5f-3cd21607c060",
   "metadata": {},
   "source": [
    "### 1) Import GPT2Tokenizer\n",
    "---\n",
    "While we built a BPETokenizer in the other files, we'll take advantage of the GPT2Tokenizer because 1) It's better (better token representations); 2) Likely more optimized, though our encoding and decoding is pretty slick now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b226c5ac-1015-4ed3-b900-5f775718cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# First import GPT2Tokenizer from HuggingFace Transformers library\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321baeb6-c031-49ba-8df1-56dc0bbbdfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e86975-13fc-4d0d-a766-d1076e6a57af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: tensor([[7120, 6291, 2420, 2925,  994,   13]])\n",
      "Decoded: Your sample text goes here.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sample text\n",
    "text = \"Your sample text goes here.\"\n",
    "encoded_input = tokenizer.encode(text, return_tensors='pt')\n",
    "decoded_output = tokenizer.decode(encoded_input[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Encoded: {encoded_input}\")\n",
    "print(f\"Decoded: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa94be-536c-405c-97c3-56fe64482889",
   "metadata": {},
   "source": [
    "### 2) Load in our data from Pytorch's existing datasets \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eabfaea1-8502-485d-b0a2-5fb5b74f456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import pytorch and the Dataloader\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e36b6585-27f8-4a6a-9ffd-07f779746553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a custom Dataset class to load in our dataset\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class TextFileDataset(Dataset):\n",
    "    def __init__(self, directory, tokenizer, max_length=8):\n",
    "        self.file_paths = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        encoded = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        return encoded.input_ids.squeeze(0), encoded.attention_mask.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4cda363-9ec5-4939-9a9f-99ae5cb866d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our wiki8_dataset\n",
    "wiki8_dataset = TextFileDataset(directory=\"..\\\\data\\\\wikitext8_680MB\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e006e6c-2d9f-4562-9a1e-8274b523dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our dataloader. Note - batch_size refers to # of 25MB chunks each batch is\n",
    "wiki8_dataloader = DataLoader(wiki8_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a5e89e0-364b-4bd2-97df-9cc34e80901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "1\n",
      "Text 1: te forces begun to gather in shim\n",
      "\n",
      "Batch 2:\n",
      "1\n",
      "Text 1: four volumes captures the full subtlety of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (input_ids, attention_mask) in enumerate(wiki8_dataloader):\n",
    "    if i >= 2:  # Just look at the first 2 batches\n",
    "        break\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"{input_ids.size(0)}\")\n",
    "    for j in range(np.min(input_ids.size(0), 10)):  # Loop through each item in the batch\n",
    "        decoded_text = tokenizer.decode(input_ids[j], skip_special_tokens=True)\n",
    "        print(f\"Text {j+1}: {decoded_text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
